{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd8f94c",
   "metadata": {},
   "source": [
    "# Local PR Summarization Pipeline\n",
    "\n",
    "This notebook implements an offline PR summarization system using:\n",
    "- Git for extracting branch diffs, commits, and changed files\n",
    "- Local Ollama API (CodeLlama) for LLM-based summarization\n",
    "- **Atomic change detection** for precise tracking of all code modifications\n",
    "- **Smart validation** to ensure no changes are missed in summaries\n",
    "- **Robust error handling** for LLM timeouts and failures\n",
    "\n",
    "## Features:\n",
    "- Extract all PR-relevant data from any two branches\n",
    "- **Parse diffs into atomic changes** (additions, deletions, modifications) with line numbers\n",
    "- Engineer optimized prompts that explicitly enumerate all changes\n",
    "- **Validate LLM output** to ensure coverage of all atomic changes\n",
    "- **Automatic re-prompting** for missing changes\n",
    "- Handle large diffs through intelligent chunking\n",
    "- Aggregate file-level summaries into comprehensive PR summary\n",
    "- **Transparent failure tracking** and placeholder summaries for failed files\n",
    "- **On-demand retry** for files that fail due to LLM timeouts\n",
    "- Fully offline and private"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dceb9cf",
   "metadata": {},
   "source": [
    "## 1. Git Data Extraction\n",
    "\n",
    "Extract all relevant PR information: branches, commits, changed files, and diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb5e3c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Git utility functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from typing import List, Dict\n",
    "\n",
    "# Get current git branch\n",
    "def get_current_branch(repo_path: str = \".\") -> str:\n",
    "    \"\"\"Get the name of the current git branch.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    return result.stdout.strip()\n",
    "\n",
    "# Get base branch (default: main)\n",
    "def get_base_branch(repo_path: str = \".\", default: str = \"main\") -> str:\n",
    "    \"\"\"\n",
    "    Get the base branch for comparison. \n",
    "    Tries to detect main/master, falls back to provided default.\n",
    "    \"\"\"\n",
    "    # Check if main exists\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"--verify\", \"main\"],\n",
    "        capture_output=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        return \"main\"\n",
    "    \n",
    "    # Check if master exists\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"--verify\", \"master\"],\n",
    "        capture_output=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        return \"master\"\n",
    "    \n",
    "    return default\n",
    "\n",
    "# Get list of changed files between base and current branch\n",
    "def get_changed_files(base: str, current: str, repo_path: str = \".\") -> List[str]:\n",
    "    \"\"\"List all files changed between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", \"--name-only\", f\"{base}...{current}\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    files = result.stdout.strip().splitlines()\n",
    "    return [f for f in files if f]  # Filter empty strings\n",
    "\n",
    "# Get commit messages between base and current branch\n",
    "def get_commit_messages(base: str, current: str, repo_path: str = \".\") -> List[str]:\n",
    "    \"\"\"Get all commit messages between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"log\", f\"{base}..{current}\", \"--pretty=format:%h - %s\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    messages = result.stdout.strip().splitlines()\n",
    "    return [m for m in messages if m]\n",
    "\n",
    "# Get diff between base and current branch\n",
    "def get_diff(base: str, current: str, repo_path: str = \".\") -> str:\n",
    "    \"\"\"Get the full diff between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", f\"{base}...{current}\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "# Get diff for a specific file\n",
    "def get_file_diff(base: str, current: str, file_path: str, repo_path: str = \".\") -> str:\n",
    "    \"\"\"Get the diff for a specific file between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", f\"{base}...{current}\", \"--\", file_path],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "print(\"✓ Git utility functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f96ad",
   "metadata": {},
   "source": [
    "## 2. Test Git Extraction\n",
    "\n",
    "Verify that we can extract all PR data from the current repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46413d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current branch: test/pr-summary-demo\n",
      "Base branch: main\n",
      "\n",
      "Changed files (6):\n",
      "  1. .github/workflows/tests.yaml\n",
      "  2. README.md\n",
      "  3. pyproject.toml\n",
      "  4. src/flask/app.py\n",
      "  5. src/flask/helpers.py\n",
      "  6. src/flask/views.py\n",
      "\n",
      "Commit messages (4):\n",
      "  - 62a77913 - Add debug print and TODO in views.py; update README for PR summarization demo\n",
      "  - 06e61ab5 - Rename variable in helpers.py for PR summarization demo\n",
      "  - 1336cc19 - Add test comment to app.py for PR summarization demo\n",
      "  - ad68a126 - drop experimental 3.13t test env\n",
      "\n",
      "Total diff size: 239 lines (9727 characters)\n",
      "Sample diff (first 500 chars):\n",
      "diff --git a/.github/workflows/tests.yaml b/.github/workflows/tests.yaml\n",
      "index 892573d8..347e90d5 100644\n",
      "--- a/.github/workflows/tests.yaml\n",
      "+++ b/.github/workflows/tests.yaml\n",
      "@@ -18,7 +18,6 @@ jobs:\n",
      "           - {name: Windows, python: '3.14', os: windows-latest}\n",
      "           - {name: Mac, python: '3.14', os: macos-latest}\n",
      "           - {python: '3.13'}\n",
      "-          - {python: '3.13t'}\n",
      "           - {python: '3.12'}\n",
      "           - {python: '3.11'}\n",
      "           - {python: '3.10'}\n",
      "diff --git a/README.md b/R...\n"
     ]
    }
   ],
   "source": [
    "# Configure repository path (adjust to your Flask repo location)\n",
    "REPO_PATH = \"../flask\"\n",
    "\n",
    "# Extract PR data\n",
    "current_branch = get_current_branch(REPO_PATH)\n",
    "base_branch = get_base_branch(REPO_PATH)\n",
    "\n",
    "print(f\"Current branch: {current_branch}\")\n",
    "print(f\"Base branch: {base_branch}\")\n",
    "\n",
    "# Get changed files\n",
    "changed_files = get_changed_files(base_branch, current_branch, REPO_PATH)\n",
    "print(f\"\\nChanged files ({len(changed_files)}):\")\n",
    "for i, file in enumerate(changed_files[:10], 1):  # Show first 10\n",
    "    print(f\"  {i}. {file}\")\n",
    "if len(changed_files) > 10:\n",
    "    print(f\"  ... and {len(changed_files) - 10} more\")\n",
    "\n",
    "# Get commit messages\n",
    "commits = get_commit_messages(base_branch, current_branch, REPO_PATH)\n",
    "print(f\"\\nCommit messages ({len(commits)}):\")\n",
    "for commit in commits[:5]:  # Show first 5\n",
    "    print(f\"  - {commit}\")\n",
    "if len(commits) > 5:\n",
    "    print(f\"  ... and {len(commits) - 5} more\")\n",
    "\n",
    "# Get diff size\n",
    "full_diff = get_diff(base_branch, current_branch, REPO_PATH)\n",
    "diff_lines = len(full_diff.splitlines())\n",
    "print(f\"\\nTotal diff size: {diff_lines} lines ({len(full_diff)} characters)\")\n",
    "print(f\"Sample diff (first 500 chars):\\n{full_diff[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba25426",
   "metadata": {},
   "source": [
    "## 3. Diff Chunking Strategy\n",
    "\n",
    "For large diffs, we need to chunk them intelligently to avoid overwhelming the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7cddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunking utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def chunk_diff_by_file(base: str, current: str, changed_files: List[str], repo_path: str = \".\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Split the diff into per-file chunks for manageable summarization.\n",
    "    Returns a dict mapping file paths to their diffs.\n",
    "    \"\"\"\n",
    "    file_diffs = {}\n",
    "    for file_path in changed_files:\n",
    "        diff = get_file_diff(base, current, file_path, repo_path)\n",
    "        if diff.strip():  # Only include files with actual changes\n",
    "            file_diffs[file_path] = diff\n",
    "    return file_diffs\n",
    "\n",
    "def truncate_large_diff(diff: str, max_lines: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Truncate very large diffs to focus on beginning and end.\n",
    "    Useful for summarization when full context isn't needed.\n",
    "    \"\"\"\n",
    "    lines = diff.splitlines()\n",
    "    if len(lines) <= max_lines:\n",
    "        return diff\n",
    "    \n",
    "    # Take first and last portions\n",
    "    half = max_lines // 2\n",
    "    truncated = lines[:half] + [\"\\n... [truncated middle section] ...\\n\"] + lines[-half:]\n",
    "    return \"\\n\".join(truncated)\n",
    "\n",
    "def should_summarize_file(file_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a file should be included in summarization.\n",
    "    Exclude generated files, lock files, config files, etc.\n",
    "    \"\"\"\n",
    "    exclude_patterns = [\n",
    "        '.github/',\n",
    "        'pyproject.toml',\n",
    "        'package-lock.json',\n",
    "        'yarn.lock',\n",
    "        'poetry.lock',\n",
    "        '.min.js',\n",
    "        '.min.css',\n",
    "        '__pycache__',\n",
    "        '.pyc',\n",
    "        '.yml',\n",
    "        '.yaml',\n",
    "        'requirements.txt',\n",
    "        'setup.py',  \n",
    "        'setup.cfg',           \n",
    "        '.gitignore',        \n",
    "        'LICENSE',    \n",
    "        'MANIFEST.in'    \n",
    "    ]\n",
    "    for pattern in exclude_patterns:\n",
    "        if pattern in file_path:\n",
    "            return False\n",
    "    return True\n",
    "print(\"✓ Chunking utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c98b1f",
   "metadata": {},
   "source": [
    "## 3.5 Atomic Diff Parsing\n",
    "\n",
    "Parse git diffs into atomic changes for precise tracking and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "289eabe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Atomic diff parser loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class AtomicChange:\n",
    "    \"\"\"Represents a single atomic change in a diff.\"\"\"\n",
    "    change_type: str  # 'addition', 'deletion', 'modification'\n",
    "    line_number: int\n",
    "    old_line: Optional[int]\n",
    "    new_line: Optional[int]\n",
    "    old_content: Optional[str]\n",
    "    new_content: Optional[str]\n",
    "    context: str  # Surrounding code context\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.change_type == 'addition':\n",
    "            return f\"Line {self.new_line}: + {self.new_content}\"\n",
    "        elif self.change_type == 'deletion':\n",
    "            return f\"Line {self.old_line}: - {self.old_content}\"\n",
    "        else:\n",
    "            return f\"Line {self.old_line}->{self.new_line}: {self.old_content} → {self.new_content}\"\n",
    "\n",
    "def parse_diff_hunks(diff: str) -> List[AtomicChange]:\n",
    "    \"\"\"\n",
    "    Parse a git diff into atomic changes with line numbers and context.\n",
    "    \n",
    "    Returns a list of AtomicChange objects, each representing a single\n",
    "    line addition, deletion, or modification.\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    lines = diff.splitlines()\n",
    "    \n",
    "    # Track current line numbers\n",
    "    old_line_num = 0\n",
    "    new_line_num = 0\n",
    "    context_buffer = []  # Store recent context lines\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        # Parse hunk headers: @@ -old_start,old_count +new_start,new_count @@\n",
    "        if line.startswith('@@'):\n",
    "            match = re.match(r'@@ -(\\d+),?\\d* \\+(\\d+),?\\d* @@', line)\n",
    "            if match:\n",
    "                old_line_num = int(match.group(1))\n",
    "                new_line_num = int(match.group(2))\n",
    "                context_buffer = []\n",
    "            continue\n",
    "        \n",
    "        # Skip file headers\n",
    "        if line.startswith('diff --git') or line.startswith('index') or \\\n",
    "           line.startswith('---') or line.startswith('+++'):\n",
    "            continue\n",
    "        \n",
    "        # Build context from last 2 unchanged lines\n",
    "        if line.startswith(' '):\n",
    "            context_buffer.append(line[1:])\n",
    "            if len(context_buffer) > 2:\n",
    "                context_buffer.pop(0)\n",
    "            old_line_num += 1\n",
    "            new_line_num += 1\n",
    "            \n",
    "        # Addition\n",
    "        elif line.startswith('+'):\n",
    "            content = line[1:].strip()\n",
    "            if content:  # Ignore empty additions\n",
    "                context = '\\n'.join(context_buffer[-2:]) if context_buffer else \"\"\n",
    "                changes.append(AtomicChange(\n",
    "                    change_type='addition',\n",
    "                    line_number=new_line_num,\n",
    "                    old_line=None,\n",
    "                    new_line=new_line_num,\n",
    "                    old_content=None,\n",
    "                    new_content=content,\n",
    "                    context=context\n",
    "                ))\n",
    "            new_line_num += 1\n",
    "            \n",
    "        # Deletion\n",
    "        elif line.startswith('-'):\n",
    "            content = line[1:].strip()\n",
    "            if content:  # Ignore empty deletions\n",
    "                context = '\\n'.join(context_buffer[-2:]) if context_buffer else \"\"\n",
    "                changes.append(AtomicChange(\n",
    "                    change_type='deletion',\n",
    "                    line_number=old_line_num,\n",
    "                    old_line=old_line_num,\n",
    "                    new_line=None,\n",
    "                    old_content=content,\n",
    "                    new_content=None,\n",
    "                    context=context\n",
    "                ))\n",
    "            old_line_num += 1\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def detect_modifications(changes: List[AtomicChange]) -> List[AtomicChange]:\n",
    "    \"\"\"\n",
    "    Post-process changes to detect modifications (deletion + addition pairs).\n",
    "    Combines adjacent deletions and additions into modification entries.\n",
    "    \"\"\"\n",
    "    if not changes:\n",
    "        return changes\n",
    "    \n",
    "    modified_changes = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(changes):\n",
    "        current = changes[i]\n",
    "        \n",
    "        # Check if this deletion is followed by an addition (likely a modification)\n",
    "        if (current.change_type == 'deletion' and \n",
    "            i + 1 < len(changes) and \n",
    "            changes[i + 1].change_type == 'addition' and\n",
    "            abs(current.line_number - changes[i + 1].line_number) <= 2):\n",
    "            \n",
    "            next_change = changes[i + 1]\n",
    "            \n",
    "            # Create a modification entry\n",
    "            modified_changes.append(AtomicChange(\n",
    "                change_type='modification',\n",
    "                line_number=current.line_number,\n",
    "                old_line=current.old_line,\n",
    "                new_line=next_change.new_line,\n",
    "                old_content=current.old_content,\n",
    "                new_content=next_change.new_content,\n",
    "                context=current.context\n",
    "            ))\n",
    "            i += 2  # Skip both the deletion and addition\n",
    "        else:\n",
    "            modified_changes.append(current)\n",
    "            i += 1\n",
    "    \n",
    "    return modified_changes\n",
    "\n",
    "def format_atomic_changes(changes: List[AtomicChange]) -> str:\n",
    "    \"\"\"\n",
    "    Format atomic changes into a clear, enumerated list for LLM prompts.\n",
    "    \"\"\"\n",
    "    if not changes:\n",
    "        return \"No atomic changes detected.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for idx, change in enumerate(changes, 1):\n",
    "        if change.change_type == 'addition':\n",
    "            formatted.append(f\"{idx}. **Added** at line {change.new_line}: `{change.new_content}`\")\n",
    "        elif change.change_type == 'deletion':\n",
    "            formatted.append(f\"{idx}. **Removed** at line {change.old_line}: `{change.old_content}`\")\n",
    "        elif change.change_type == 'modification':\n",
    "            formatted.append(\n",
    "                f\"{idx}. **Changed** at line {change.old_line}: \"\n",
    "                f\"`{change.old_content}` → `{change.new_content}`\"\n",
    "            )\n",
    "    \n",
    "    return '\\n'.join(formatted)\n",
    "\n",
    "print(\"✓ Atomic diff parser loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422cc95",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering\n",
    "\n",
    "Create effective prompts for the LLM to generate high-quality summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c2b329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt templates created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_file_summary_prompt(file_path: str, diff: str, max_diff_lines: int = 150) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for summarizing a single file's changes.\n",
    "    Now includes enumerated atomic changes for precise tracking.\n",
    "    The summary must be a concise, comprehensive paragraph (not a bullet list).\n",
    "    \"\"\"\n",
    "    # Parse atomic changes from diff\n",
    "    atomic_changes = parse_diff_hunks(diff)\n",
    "    atomic_changes = detect_modifications(atomic_changes)\n",
    "    \n",
    "    # Format atomic changes\n",
    "    changes_list = format_atomic_changes(atomic_changes)\n",
    "    change_count = len(atomic_changes)\n",
    "    \n",
    "    # Also include truncated diff for additional context\n",
    "    truncated_diff = truncate_large_diff(diff, max_diff_lines)\n",
    "    \n",
    "    prompt = f\"\"\"Summarize the code changes for this file. You must mention ALL {change_count} changes listed below.\n",
    "\n",
    "File: {file_path}\n",
    "\n",
    "Atomic Changes ({change_count} total):\n",
    "{changes_list}\n",
    "\n",
    "Full Diff Context:\n",
    "```\n",
    "{truncated_diff}\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "- Describe ALL {change_count} atomic changes listed above\n",
    "- Be specific: mention variable names, function names, line additions/deletions\n",
    "- **Write a single concise paragraph (1-2 sentences), not a bullet list**\n",
    "- Do not infer or hallucinate changes not shown above\n",
    "\n",
    "Summary (concise paragraph):\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_overall_summary_prompt(\n",
    "    base_branch: str, \n",
    "    current_branch: str, \n",
    "    commits: list, \n",
    "    changed_files: list,\n",
    "    file_summaries: list\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for generating an overall PR summary from file-level summaries.\n",
    "    \"\"\"\n",
    "    commits_text = \"\\n\".join(f\"  - {commit}\" for commit in commits[:10])\n",
    "    if len(commits) > 10:\n",
    "        commits_text += f\"\\n  ... and {len(commits) - 10} more commits\"\n",
    "    \n",
    "    files_text = \"\\n\".join(f\"  - {file}\" for file in changed_files[:15])\n",
    "    if len(changed_files) > 15:\n",
    "        files_text += f\"\\n  ... and {len(changed_files) - 15} more files\"\n",
    "    \n",
    "    summaries_text = \"\\n\\n\".join(f\"{i+1}. {summary}\" for i, summary in enumerate(file_summaries))\n",
    "    \n",
    "    prompt = f\"\"\"Summarize this pull request based only on the information below. Be concise (2-3 sentences total).\n",
    "\n",
    "Branch: {current_branch} → {base_branch}\n",
    "\n",
    "Commits ({len(commits)}):\n",
    "\n",
    "\n",
    "Commits: {len(commits)}\n",
    "Changed files {len(changed_files)}\n",
    "{files_text}\n",
    "\n",
    "File summaries:\n",
    "\n",
    "{summaries_text}\n",
    "\n",
    "Provide a brief PR summary covering: purpose, main changes, and impact. Keep it under 3 sentences total.\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"✓ Prompt templates created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46db0b",
   "metadata": {},
   "source": [
    "## 4.5 Summary Validation\n",
    "\n",
    "Verify that LLM summaries cover all atomic changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73f72999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Summary validation utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def validate_summary_coverage(\n",
    "    summary: str, \n",
    "    atomic_changes: List[AtomicChange],\n",
    "    file_path: str\n",
    ") -> Tuple[bool, List[AtomicChange], Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Validate that the LLM summary mentions all atomic changes.\n",
    "    \n",
    "    Returns:\n",
    "        - is_complete: bool indicating if all changes are covered\n",
    "        - missing_changes: List of AtomicChange objects not mentioned in summary\n",
    "        - metrics: Dict with coverage statistics\n",
    "    \"\"\"\n",
    "    if not summary or not atomic_changes:\n",
    "        return True, [], {\"total\": 0, \"mentioned\": 0, \"coverage\": 100.0}\n",
    "    \n",
    "    summary_lower = summary.lower()\n",
    "    missing_changes = []\n",
    "    mentioned_count = 0\n",
    "    \n",
    "    for change in atomic_changes:\n",
    "        is_mentioned = False\n",
    "        \n",
    "        # Check if key content from the change appears in summary\n",
    "        if change.change_type == 'addition' and change.new_content:\n",
    "            # Look for the added content or key keywords from it\n",
    "            content_keywords = extract_keywords(change.new_content)\n",
    "            if any(kw in summary_lower for kw in content_keywords):\n",
    "                is_mentioned = True\n",
    "                \n",
    "        elif change.change_type == 'deletion' and change.old_content:\n",
    "            # Look for the deleted content or removal mention\n",
    "            content_keywords = extract_keywords(change.old_content)\n",
    "            if any(kw in summary_lower for kw in content_keywords) or \\\n",
    "               'remov' in summary_lower or 'delet' in summary_lower:\n",
    "                is_mentioned = True\n",
    "                \n",
    "        elif change.change_type == 'modification':\n",
    "            # Look for old or new content, or change/rename keywords\n",
    "            old_keywords = extract_keywords(change.old_content) if change.old_content else []\n",
    "            new_keywords = extract_keywords(change.new_content) if change.new_content else []\n",
    "            if any(kw in summary_lower for kw in old_keywords + new_keywords) or \\\n",
    "               'chang' in summary_lower or 'modif' in summary_lower or 'renam' in summary_lower:\n",
    "                is_mentioned = True\n",
    "        \n",
    "        if is_mentioned:\n",
    "            mentioned_count += 1\n",
    "        else:\n",
    "            missing_changes.append(change)\n",
    "    \n",
    "    total = len(atomic_changes)\n",
    "    coverage = (mentioned_count / total * 100) if total > 0 else 100.0\n",
    "    \n",
    "    metrics = {\n",
    "        \"file\": file_path,\n",
    "        \"total_changes\": total,\n",
    "        \"mentioned_changes\": mentioned_count,\n",
    "        \"missing_changes\": len(missing_changes),\n",
    "        \"coverage_percent\": coverage\n",
    "    }\n",
    "    \n",
    "    is_complete = coverage >= 80.0  # 80% threshold for acceptable coverage\n",
    "    \n",
    "    return is_complete, missing_changes, metrics\n",
    "\n",
    "def extract_keywords(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract meaningful keywords from code text for validation.\n",
    "    Focuses on identifiers, function names, and significant tokens.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Remove common noise and split into tokens\n",
    "    text = text.lower()\n",
    "    # Extract words that are likely identifiers (alphanumeric + underscore)\n",
    "    words = re.findall(r'\\b[a-z_][a-z0-9_]*\\b', text)\n",
    "    \n",
    "    # Filter out common keywords and very short words\n",
    "    common_keywords = {'if', 'else', 'for', 'while', 'return', 'def', 'class', \n",
    "                      'import', 'from', 'as', 'in', 'is', 'and', 'or', 'not', 'the', 'a', 'an'}\n",
    "    keywords = [w for w in words if len(w) > 2 and w not in common_keywords]\n",
    "    \n",
    "    return keywords[:5]  # Return top 5 most significant keywords\n",
    "\n",
    "def create_reprompt_for_missing_changes(\n",
    "    file_path: str,\n",
    "    original_summary: str,\n",
    "    missing_changes: List[AtomicChange]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a focused re-prompt for changes that were not covered in the initial summary.\n",
    "    \"\"\"\n",
    "    changes_list = format_atomic_changes(missing_changes)\n",
    "    \n",
    "    prompt = f\"\"\"Your previous summary for {file_path} missed some changes. Please describe these specific changes:\n",
    "\n",
    "Missing Changes:\n",
    "{changes_list}\n",
    "\n",
    "Previous Summary:\n",
    "{original_summary}\n",
    "\n",
    "Provide a brief description (1 sentence) of the missing changes above:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"✓ Summary validation utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615895ca",
   "metadata": {},
   "source": [
    "## 5. Ollama API Integration\n",
    "\n",
    "Connect to the local Ollama server and send prompts for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e35f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama server is running!\n",
      "  Available models: codellama:7b-instruct, llama3:latest\n",
      "  codellama:7b-instruct is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"codellama:7b-instruct\"\n",
    "\n",
    "def call_ollama(prompt: str, model: str = MODEL_NAME, temperature: float = 0.3, timeout: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Send a prompt to the local Ollama API and return the generated response.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt for the LLM\n",
    "        model: Model name (default: codellama)\n",
    "        temperature: Sampling temperature (lower = more focused, higher = more creative)\n",
    "        timeout: Request timeout in seconds (default: 200)\n",
    "    \n",
    "    Returns:\n",
    "        The generated text response, or None if the request fails\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"num_predict\": 150  # Max tokens for concise summary\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result.get(\"response\", \"\").strip()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"LLM request timed out after {timeout} seconds\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"LLM request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_ollama_connection() -> bool:\n",
    "    \"\"\"Test if Ollama server is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            model_names = [m.get(\"name\", \"\") for m in models]\n",
    "            print(f\"✓ Ollama server is running!\")\n",
    "            print(f\"  Available models: {', '.join(model_names)}\")\n",
    "            \n",
    "            if MODEL_NAME in model_names or any(MODEL_NAME in name for name in model_names):\n",
    "                print(f\"  {MODEL_NAME} is available\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"  {MODEL_NAME} not found. Available: {model_names}\")\n",
    "                return False\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" Cannot connect to Ollama server: {e}\")\n",
    "        print(f\"  Make sure Ollama is running (start the Ollama desktop app)\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "test_ollama_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07659d",
   "metadata": {},
   "source": [
    "## 6. Complete PR Summarization Pipeline\n",
    "\n",
    "Orchestrate all steps: extract data → chunk → summarize → aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad1699a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PR summarization pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "def summarize_pr(\n",
    "    repo_path: str = \".\",\n",
    "    base_branch: str = None,\n",
    "    current_branch: str = None,\n",
    "    max_files_to_summarize: int = 10,\n",
    "    enable_validation: bool = True,\n",
    "    retry_missing: bool = True,\n",
    "    llm_timeout: int = 200,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Complete PR summarization pipeline with atomic change tracking and validation.\n",
    "    Now includes robust error handling for LLM failures.\n",
    "    \n",
    "    Args:\n",
    "        repo_path: Path to git repository\n",
    "        base_branch: Base branch for comparison (auto-detected if None)\n",
    "        current_branch: Current branch (auto-detected if None)\n",
    "        max_files_to_summarize: Maximum number of files to process\n",
    "        enable_validation: Whether to validate summaries cover all changes\n",
    "        retry_missing: Whether to re-prompt for missing changes\n",
    "        llm_timeout: Timeout in seconds for LLM requests\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "        - base_branch: str\n",
    "        - current_branch: str\n",
    "        - commits: List[str]\n",
    "        - changed_files: List[str]\n",
    "        - file_summaries: Dict[str, str]\n",
    "        - failed_files: List[str] - files that failed to summarize\n",
    "        - file_metrics: Dict[str, Dict] - validation metrics per file\n",
    "        - overall_summary: str\n",
    "        - repo_path: str - for use in retry functions\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"PR SUMMARIZATION PIPELINE (with atomic change tracking)\")\n",
    "    \n",
    "    # Step 1: Extract git data\n",
    "    if verbose:\n",
    "        print(\"\\n[1/5] Extracting PR data from git...\")\n",
    "    \n",
    "    if not current_branch:\n",
    "        current_branch = get_current_branch(repo_path)\n",
    "    if not base_branch:\n",
    "        base_branch = get_base_branch(repo_path)\n",
    "    \n",
    "    commits = get_commit_messages(base_branch, current_branch, repo_path)\n",
    "    changed_files = get_changed_files(base_branch, current_branch, repo_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Branch: {current_branch} → {base_branch}\")\n",
    "        print(f\"  Commits: {len(commits)}\")\n",
    "        print(f\"  Changed files: {len(changed_files)}\")\n",
    "    \n",
    "    if not changed_files:\n",
    "        return {\n",
    "            \"base_branch\": base_branch,\n",
    "            \"current_branch\": current_branch,\n",
    "            \"commits\": commits,\n",
    "            \"changed_files\": changed_files,\n",
    "            \"file_summaries\": {},\n",
    "            \"failed_files\": [],\n",
    "            \"file_metrics\": {},\n",
    "            \"overall_summary\": \"No changes detected between branches.\",\n",
    "            \"repo_path\": repo_path\n",
    "        }\n",
    "    \n",
    "    # Step 2: Chunk diffs by file\n",
    "    if verbose:\n",
    "        print(f\"\\n[2/5] Chunking diffs by file...\")\n",
    "    \n",
    "    file_diffs = chunk_diff_by_file(base_branch, current_branch, changed_files, repo_path)\n",
    "    \n",
    "    # Filter files to summarize\n",
    "    files_to_summarize = [\n",
    "        f for f in changed_files \n",
    "        if should_summarize_file(f) and f in file_diffs\n",
    "    ][:max_files_to_summarize]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Files to summarize: {len(files_to_summarize)}\")\n",
    "    \n",
    "    # Step 3: Summarize each file with atomic change tracking\n",
    "    if verbose:\n",
    "        print(f\"\\n[3/5] Generating file-level summaries with atomic change tracking...\")\n",
    "    \n",
    "    file_summaries = {}\n",
    "    failed_files = []\n",
    "    file_metrics = {}\n",
    "    \n",
    "    for i, file_path in enumerate(files_to_summarize, 1):\n",
    "        if verbose:\n",
    "            print(f\"  [{i}/{len(files_to_summarize)}] {file_path}...\")\n",
    "        \n",
    "        # Parse atomic changes\n",
    "        diff = file_diffs[file_path]\n",
    "        atomic_changes = parse_diff_hunks(diff)\n",
    "        atomic_changes = detect_modifications(atomic_changes)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"      → {len(atomic_changes)} atomic changes detected\")\n",
    "        \n",
    "        # Generate initial summary\n",
    "        prompt = create_file_summary_prompt(file_path, diff)\n",
    "        summary = call_ollama(prompt, timeout=llm_timeout)\n",
    "        \n",
    "        # Handle LLM failures\n",
    "        if not summary:\n",
    "            if verbose:\n",
    "                print(f\"       Failed to generate summary (timeout/error)\")\n",
    "            failed_files.append(file_path)\n",
    "            file_summaries[file_path] = \" Summary could not be generated for this file due to LLM timeout or error.\"\n",
    "            continue\n",
    "        \n",
    "        # Step 4: Validate coverage\n",
    "        if enable_validation and atomic_changes:\n",
    "            is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "                summary, atomic_changes, file_path\n",
    "            )\n",
    "            file_metrics[file_path] = metrics\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"      Coverage: {metrics['coverage_percent']:.1f}% \"\n",
    "                      f\"({metrics['mentioned_changes']}/{metrics['total_changes']} changes)\")\n",
    "            \n",
    "            # Step 5: Re-prompt for missing changes if enabled\n",
    "            if not is_complete and retry_missing and missing_changes:\n",
    "                if verbose:\n",
    "                    print(f\"      Re-prompting for {len(missing_changes)} missing changes...\")\n",
    "                \n",
    "                reprompt = create_reprompt_for_missing_changes(file_path, summary, missing_changes)\n",
    "                additional_summary = call_ollama(reprompt, timeout=llm_timeout)\n",
    "                \n",
    "                if additional_summary:\n",
    "                    summary = f\"{summary} {additional_summary}\"\n",
    "                    \n",
    "                    # Re-validate\n",
    "                    is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "                        summary, atomic_changes, file_path\n",
    "                    )\n",
    "                    file_metrics[file_path] = metrics\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"      Updated coverage: {metrics['coverage_percent']:.1f}%\")\n",
    "        \n",
    "        file_summaries[file_path] = summary\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[4/5] Generating overall PR summary...\")\n",
    "    \n",
    "    # Step 6: Generate overall summary (exclude failed files from summary generation)\n",
    "    successful_summaries = {\n",
    "        file: summary for file, summary in file_summaries.items()\n",
    "        if file not in failed_files\n",
    "    }\n",
    "    \n",
    "    if not successful_summaries:\n",
    "        overall_summary = \"No files could be summarized successfully.\"\n",
    "    else:\n",
    "        summary_list = [f\"{file}: {summary}\" for file, summary in successful_summaries.items()]\n",
    "        overall_prompt = create_overall_summary_prompt(\n",
    "            base_branch, \n",
    "            current_branch, \n",
    "            commits, \n",
    "            changed_files,\n",
    "            summary_list\n",
    "        )\n",
    "        overall_summary = call_ollama(overall_prompt, timeout=llm_timeout)\n",
    "        if not overall_summary:\n",
    "            overall_summary = \"Error generating overall summary due to LLM timeout or error.\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[5/5] Pipeline complete!\")\n",
    "        print(f\"  Successfully summarized: {len(successful_summaries)} files\")\n",
    "        if failed_files:\n",
    "            print(f\"   Failed to summarize: {len(failed_files)} files\")\n",
    "        if file_metrics:\n",
    "            avg_coverage = sum(m['coverage_percent'] for m in file_metrics.values()) / len(file_metrics)\n",
    "            print(f\"  Average change coverage: {avg_coverage:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"base_branch\": base_branch,\n",
    "        \"current_branch\": current_branch,\n",
    "        \"commits\": commits,\n",
    "        \"changed_files\": changed_files,\n",
    "        \"file_summaries\": file_summaries,\n",
    "        \"failed_files\": failed_files,\n",
    "        \"file_metrics\": file_metrics,\n",
    "        \"overall_summary\": overall_summary,\n",
    "        \"repo_path\": repo_path\n",
    "    }\n",
    "\n",
    "print(\"✓ PR summarization pipeline ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa41c3b",
   "metadata": {},
   "source": [
    "## 6.5 On-Demand Retry for Failed Files\n",
    "\n",
    "Re-summarize specific files that failed due to LLM timeout/error, with configurable timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbb4edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ On-demand retry functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def summarize_failed_file(\n",
    "    result: Dict[str, any],\n",
    "    file_path: str,\n",
    "    timeout: int = 600,\n",
    "    enable_validation: bool = True,\n",
    "    retry_missing: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Retry summarization for a specific file that previously failed.\n",
    "    \n",
    "    This function allows users to manually retry files that failed due to LLM\n",
    "    timeout or errors, potentially with a longer timeout value.\n",
    "    \n",
    "    Args:\n",
    "        result: The PR summarization result dict from summarize_pr\n",
    "        file_path: The specific file to retry (must be in failed_files list)\n",
    "        timeout: Timeout in seconds for LLM requests (default: 600 for longer wait)\n",
    "        enable_validation: Whether to validate summary coverage\n",
    "        retry_missing: Whether to re-prompt for missing changes\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Updated result dictionary with new summary for the specified file\n",
    "    \n",
    "    Usage:\n",
    "        # After initial run, if helpers.py failed:\n",
    "        result = summarize_failed_file(result, \"src/flask/helpers.py\", timeout=600)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if file_path not in result.get('failed_files', []):\n",
    "        if verbose:\n",
    "            print(f\" {file_path} is not in the failed files list.\")\n",
    "            print(f\"   Failed files: {result.get('failed_files', [])}\")\n",
    "        return result\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n Retrying summarization for: {file_path}\")\n",
    "        print(f\"   Timeout: {timeout} seconds\")\n",
    "    \n",
    "    # Extract necessary data from result\n",
    "    repo_path = result['repo_path']\n",
    "    base_branch = result['base_branch']\n",
    "    current_branch = result['current_branch']\n",
    "    \n",
    "    # Get the diff for this file\n",
    "    diff = get_file_diff(base_branch, current_branch, file_path, repo_path)\n",
    "    \n",
    "    if not diff.strip():\n",
    "        if verbose:\n",
    "            print(f\"    No diff found for {file_path}\")\n",
    "        return result\n",
    "    \n",
    "    # Parse atomic changes\n",
    "    atomic_changes = parse_diff_hunks(diff)\n",
    "    atomic_changes = detect_modifications(atomic_changes)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   → {len(atomic_changes)} atomic changes detected\")\n",
    "    \n",
    "    # Generate summary with specified timeout\n",
    "    prompt = create_file_summary_prompt(file_path, diff)\n",
    "    summary = call_ollama(prompt, timeout=timeout)\n",
    "    \n",
    "    if not summary:\n",
    "        if verbose:\n",
    "            print(f\"    Summary still failed with {timeout}s timeout\")\n",
    "        return result\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ✓ Summary generated successfully!\")\n",
    "    \n",
    "    # Validate coverage if enabled\n",
    "    if enable_validation and atomic_changes:\n",
    "        is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "            summary, atomic_changes, file_path\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Coverage: {metrics['coverage_percent']:.1f}% \"\n",
    "                  f\"({metrics['mentioned_changes']}/{metrics['total_changes']} changes)\")\n",
    "        \n",
    "        # Re-prompt for missing changes if enabled\n",
    "        if not is_complete and retry_missing and missing_changes:\n",
    "            if verbose:\n",
    "                print(f\"   Re-prompting for {len(missing_changes)} missing changes...\")\n",
    "            \n",
    "            reprompt = create_reprompt_for_missing_changes(file_path, summary, missing_changes)\n",
    "            additional_summary = call_ollama(reprompt, timeout=timeout)\n",
    "            \n",
    "            if additional_summary:\n",
    "                summary = f\"{summary} {additional_summary}\"\n",
    "                \n",
    "                # Re-validate\n",
    "                is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "                    summary, atomic_changes, file_path\n",
    "                )\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"   Updated coverage: {metrics['coverage_percent']:.1f}%\")\n",
    "        \n",
    "        # Update metrics\n",
    "        result['file_metrics'][file_path] = metrics\n",
    "    \n",
    "    # Update result in-place\n",
    "    result['file_summaries'][file_path] = summary\n",
    "    result['failed_files'].remove(file_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ✓ Successfully updated summary for {file_path}\")\n",
    "        print(f\"   Remaining failed files: {len(result['failed_files'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def list_failed_files(result: Dict[str, any]):\n",
    "    \"\"\"\n",
    "    Display a list of all files that failed to summarize.\n",
    "    \n",
    "    Args:\n",
    "        result: The PR summarization result dict from summarize_pr\n",
    "    \"\"\"\n",
    "    failed = result.get('failed_files', [])\n",
    "    \n",
    "    if not failed:\n",
    "        print(\"✓ All files were summarized successfully!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n {len(failed)} file(s) failed to summarize:\\n\")\n",
    "    for i, file_path in enumerate(failed, 1):\n",
    "        print(f\"  {i}. {file_path}\")\n",
    "    \n",
    "    print(f\"\\nTo retry a specific file, use:\")\n",
    "    print(f\"  result = summarize_failed_file(result, '<file_path>', timeout=600)\")\n",
    "\n",
    "print(\"✓ On-demand retry functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3843d",
   "metadata": {},
   "source": [
    "## 7. Run PR Summarization\n",
    "\n",
    "Execute the pipeline on the current repository and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "321b7578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR SUMMARIZATION PIPELINE (with atomic change tracking)\n",
      "\n",
      "[1/5] Extracting PR data from git...\n",
      "  Branch: test/pr-summary-demo → main\n",
      "  Commits: 4\n",
      "  Changed files: 6\n",
      "\n",
      "[2/5] Chunking diffs by file...\n",
      "  Files to summarize: 4\n",
      "\n",
      "[3/5] Generating file-level summaries with atomic change tracking...\n",
      "  [1/4] README.md...\n",
      "      → 1 atomic changes detected\n",
      "      Coverage: 100.0% (1/1 changes)\n",
      "  [2/4] src/flask/app.py...\n",
      "      → 1 atomic changes detected\n",
      "      Coverage: 100.0% (1/1 changes)\n",
      "  [3/4] src/flask/helpers.py...\n",
      "      → 25 atomic changes detected\n",
      "LLM request timed out after 200 seconds\n",
      "       Failed to generate summary (timeout/error)\n",
      "  [4/4] src/flask/views.py...\n",
      "      → 1 atomic changes detected\n",
      "      Coverage: 100.0% (1/1 changes)\n",
      "\n",
      "[4/5] Generating overall PR summary...\n",
      "\n",
      "[5/5] Pipeline complete!\n",
      "  Successfully summarized: 3 files\n",
      "   Failed to summarize: 1 files\n",
      "  Average change coverage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Run the PR summarization pipeline\n",
    "result = summarize_pr(\n",
    "    repo_path=REPO_PATH,\n",
    "    max_files_to_summarize=5,  # Limit for testing; increase for full analysis\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ecf2db",
   "metadata": {},
   "source": [
    "## 8. Display Results\n",
    "\n",
    "Format and display the PR summary in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "033358a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR SUMMARY\n",
      "\n",
      "Branch: test/pr-summary-demo → main\n",
      "Commits: 4\n",
      "Changed files: 6\n",
      "\n",
      "Recent Commits:\n",
      "   62a77913 - Add debug print and TODO in views.py; update README for PR summarization demo\n",
      "   06e61ab5 - Rename variable in helpers.py for PR summarization demo\n",
      "   1336cc19 - Add test comment to app.py for PR summarization demo\n",
      "   ad68a126 - drop experimental 3.13t test env\n",
      "✓ Successfully Summarized Files (3):\n",
      "\n",
      "🔹 README.md\n",
      "    1/1 changes covered (100.0%)\n",
      "   The README.md file has been modified to include a new line with the text \"Test change for PR summarization demo.\" at line 55, which adds a new line to the file.\n",
      "\n",
      "🔹 src/flask/app.py\n",
      "    1/1 changes covered (100.0%)\n",
      "   This file contains one atomic change: the addition of a comment at line 1. The comment reads \"#Test: Added comment for PR summarization demo\". This comment was added to the file as part of a pull request and serves as a demonstration of how changes can be summarized using Git's diff output.\n",
      "\n",
      "🔹 src/flask/views.py\n",
      "    1/1 changes covered (100.0%)\n",
      "   The code change in `src/flask/views.py` adds a new line of code at line 104, which prints the message \"Debug: Entered some_view\" to the console when the view function is entered. This change was made by adding the print statement and setting the `init_every_request` class attribute to `True`.\n",
      "  Failed to Summarize (1):\n",
      "\n",
      " src/flask/helpers.py\n",
      "    Summary could not be generated for this file due to LLM timeout or error.\n",
      "  To retry a failed file with longer timeout:\n",
      "   result = summarize_failed_file(result, '<file_path>', timeout=600)\n",
      " Validation Summary:\n",
      "   Total atomic changes tracked: 3\n",
      "   Changes mentioned in summaries: 3\n",
      "   Average coverage: 100.0%\n",
      " Overall PR Summary:\n",
      "This pull request modifies several files in the Flask project, including the README file, app.py, and views.py. The main change is the addition of a new line to the README file, which includes the text \"Test change for PR summarization demo.\" Additionally, a comment has been added to the app.py file, which reads \"#Test: Added comment for PR summarization demo\". Finally, a print statement has been added to the views.py file at line 104, which prints the message \"Debug: Entered some_view\" when the view function is entered. Overall, this pull request serves as a demonstration of how changes can be summarized using Git's diff\n"
     ]
    }
   ],
   "source": [
    "def display_pr_summary(result: Dict[str, any]):\n",
    "    \"\"\"Display the PR summary in a formatted, readable way with validation metrics and failure reporting.\"\"\"\n",
    "    \n",
    "    print(\"PR SUMMARY\")\n",
    "    \n",
    "    print(f\"\\nBranch: {result['current_branch']} → {result['base_branch']}\")\n",
    "    print(f\"Commits: {len(result['commits'])}\")\n",
    "    print(f\"Changed files: {len(result['changed_files'])}\")\n",
    "    \n",
    "    # Display commit messages\n",
    "    print(f\"\\nRecent Commits:\")\n",
    "    for commit in result['commits'][:5]:\n",
    "        print(f\"   {commit}\")\n",
    "    if len(result['commits']) > 5:\n",
    "        print(f\"   ... and {len(result['commits']) - 5} more\")\n",
    "    \n",
    "    # Separate successful and failed summaries\n",
    "    failed_files = result.get('failed_files', [])\n",
    "    successful_files = [f for f in result['file_summaries'].keys() if f not in failed_files]\n",
    "    \n",
    "    # Display file-level summaries with validation metrics\n",
    "    if successful_files:\n",
    "        print(f\"✓ Successfully Summarized Files ({len(successful_files)}):\")\n",
    "        for file_path in successful_files:\n",
    "            summary = result['file_summaries'][file_path]\n",
    "            print(f\"\\n🔹 {file_path}\")\n",
    "            \n",
    "            # Show validation metrics if available\n",
    "            if 'file_metrics' in result and file_path in result['file_metrics']:\n",
    "                metrics = result['file_metrics'][file_path]\n",
    "                print(f\"    {metrics['mentioned_changes']}/{metrics['total_changes']} changes covered \"\n",
    "                      f\"({metrics['coverage_percent']:.1f}%)\")\n",
    "            \n",
    "            print(f\"   {summary}\")\n",
    "    \n",
    "    # Display failed files prominently\n",
    "    if failed_files:\n",
    "        print(f\"  Failed to Summarize ({len(failed_files)}):\")\n",
    "        for file_path in failed_files:\n",
    "            print(f\"\\n {file_path}\")\n",
    "            print(f\"   {result['file_summaries'].get(file_path, 'No placeholder found')}\")\n",
    "        \n",
    "        print(\"  To retry a failed file with longer timeout:\")\n",
    "        print(\"   result = summarize_failed_file(result, '<file_path>', timeout=600)\")\n",
    "    \n",
    "    # Show overall validation statistics\n",
    "    if 'file_metrics' in result and result['file_metrics']:\n",
    "        print(\" Validation Summary:\")\n",
    "        total_changes = sum(m['total_changes'] for m in result['file_metrics'].values())\n",
    "        total_mentioned = sum(m['mentioned_changes'] for m in result['file_metrics'].values())\n",
    "        avg_coverage = sum(m['coverage_percent'] for m in result['file_metrics'].values()) / len(result['file_metrics'])\n",
    "        \n",
    "        print(f\"   Total atomic changes tracked: {total_changes}\")\n",
    "        print(f\"   Changes mentioned in summaries: {total_mentioned}\")\n",
    "        print(f\"   Average coverage: {avg_coverage:.1f}%\")\n",
    "    \n",
    "    # Display overall summary\n",
    "    print(\" Overall PR Summary:\")\n",
    "    print(result['overall_summary'])\n",
    "    \n",
    "# Display the results\n",
    "display_pr_summary(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f6d8d",
   "metadata": {},
   "source": [
    "## 9. Export Summary (Optional)\n",
    "\n",
    "Save the PR summary to a markdown file for documentation or review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a1de344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PR summary exported to ../pr_summary-demo01.md\n",
      "   Note: 1 file(s) failed - see export for retry instructions\n",
      "✓ Export complete.\n"
     ]
    }
   ],
   "source": [
    "def export_summary_to_markdown(result: Dict[str, any], output_path: str = \"pr_summary.md\"):\n",
    "    \"\"\"Export the PR summary to a markdown file with validation metrics and failure reporting.\"\"\"\n",
    "    \n",
    "    # Separate successful and failed files\n",
    "    failed_files = result.get('failed_files', [])\n",
    "    successful_files = [f for f in result['file_summaries'].keys() if f not in failed_files]\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# PR Summary: {result['current_branch']} → {result['base_branch']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"**Commits:** {len(result['commits'])}  \\n\")\n",
    "        f.write(f\"**Total Changed Files:** {len(result['changed_files'])}  \\n\")\n",
    "        f.write(f\"**Successfully Summarized:** {len(successful_files)}  \\n\")\n",
    "        \n",
    "        if failed_files:\n",
    "            f.write(f\"** Failed to Summarize:** {len(failed_files)}  \\n\")\n",
    "        \n",
    "        # Add validation metrics if available\n",
    "        if 'file_metrics' in result and result['file_metrics']:\n",
    "            total_changes = sum(m['total_changes'] for m in result['file_metrics'].values())\n",
    "            total_mentioned = sum(m['mentioned_changes'] for m in result['file_metrics'].values())\n",
    "            avg_coverage = sum(m['coverage_percent'] for m in result['file_metrics'].values()) / len(result['file_metrics'])\n",
    "            f.write(f\"**Atomic Changes Tracked:** {total_changes}  \\n\")\n",
    "            f.write(f\"**Coverage:** {avg_coverage:.1f}% ({total_mentioned}/{total_changes} changes mentioned)  \\n\")\n",
    "        \n",
    "        f.write(\"\\n---\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Commits\\n\\n\")\n",
    "        for commit in result['commits']:\n",
    "            f.write(f\"- {commit}\\n\")\n",
    "        \n",
    "        # Export successful summaries\n",
    "        if successful_files:\n",
    "            f.write(\"\\n## ✓ Successfully Summarized Files\\n\\n\")\n",
    "            for file_path in successful_files:\n",
    "                summary = result['file_summaries'][file_path]\n",
    "                f.write(f\"### `{file_path}`\\n\\n\")\n",
    "                \n",
    "                # Include metrics if available\n",
    "                if 'file_metrics' in result and file_path in result['file_metrics']:\n",
    "                    metrics = result['file_metrics'][file_path]\n",
    "                    f.write(f\"**Changes:** {metrics['mentioned_changes']}/{metrics['total_changes']} \"\n",
    "                           f\"({metrics['coverage_percent']:.1f}% coverage)  \\n\\n\")\n",
    "                \n",
    "                f.write(f\"{summary}\\n\\n\")\n",
    "        \n",
    "        # Export failed files section\n",
    "        if failed_files:\n",
    "            f.write(\"\\n##  Files That Could Not Be Summarized\\n\\n\")\n",
    "            f.write(\"The following files failed to generate summaries due to LLM timeout or errors. \")\n",
    "            f.write(\"You can retry these files using the `summarize_failed_file()` function with a longer timeout.\\n\\n\")\n",
    "            \n",
    "            for file_path in failed_files:\n",
    "                f.write(f\"### `{file_path}`\\n\\n\")\n",
    "                f.write(f\">  {result['file_summaries'].get(file_path, 'Summary could not be generated.')}\\n\\n\")\n",
    "                f.write(f\"**To retry:** `result = summarize_failed_file(result, '{file_path}', timeout=600)`\\n\\n\")\n",
    "        \n",
    "        f.write(\"\\n---\\n\\n\")\n",
    "        f.write(\"## Overall Summary\\n\\n\")\n",
    "        f.write(f\"{result['overall_summary']}\\n\")\n",
    "        \n",
    "        # Add footer with retry instructions if there are failures\n",
    "        if failed_files:\n",
    "            f.write(\"\\n---\\n\\n\")\n",
    "            f.write(\"###  Retry Instructions\\n\\n\")\n",
    "            f.write(\"To retry failed file summaries with a longer timeout (e.g., 600 seconds):\\n\\n\")\n",
    "            f.write(\"```python\\n\")\n",
    "            f.write(\"# Retry a specific file\\n\")\n",
    "            f.write(\"result = summarize_failed_file(result, '<file_path>', timeout=600)\\n\\n\")\n",
    "            f.write(\"# Re-export after successful retry\\n\")\n",
    "            f.write(f\"export_summary_to_markdown(result, output_path='{output_path}')\\n\")\n",
    "            f.write(\"```\\n\")\n",
    "    \n",
    "    print(f\"✓ PR summary exported to {output_path}\")\n",
    "    if failed_files:\n",
    "        print(f\"   Note: {len(failed_files)} file(s) failed - see export for retry instructions\")\n",
    "\n",
    "# Uncomment to export\n",
    "export_summary_to_markdown(result, output_path=\"../pr_summary-demo01.md\")\n",
    "print(f\"✓ Export complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9ea7d",
   "metadata": {},
   "source": [
    "## 10. Test Atomic Change Detection\n",
    "\n",
    "Verify that the atomic parser correctly identifies all changes in a sample diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c109a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing atomic change detection for: src/flask/helpers.py\n",
      "\n",
      "Detected 25 atomic changes:\n",
      "\n",
      "1. DELETION\n",
      "   Line 32: - val = os.environ.get(\"FLASK_DEBUG\")\n",
      "\n",
      "2. MODIFICATION\n",
      "   Line 33→32\n",
      "   OLD: return bool(val and val.lower() not in {\"0\", \"false\", \"no\"})\n",
      "   NEW: val_demo = os.environ.get(\"FLASK_DEBUG\")\n",
      "\n",
      "3. ADDITION\n",
      "   Line 33: + return bool(val_demo and val_demo.lower() not in {\"0\", \"false\", \"no\"})\n",
      "\n",
      "4. MODIFICATION\n",
      "   Line 43→43\n",
      "   OLD: val = os.environ.get(\"FLASK_SKIP_DOTENV\")\n",
      "   NEW: val_demo = os.environ.get(\"FLASK_SKIP_DOTENV\")\n",
      "\n",
      "5. MODIFICATION\n",
      "   Line 45→45\n",
      "   OLD: if not val:\n",
      "   NEW: if not val_demo:\n",
      "\n",
      "6. MODIFICATION\n",
      "   Line 48→48\n",
      "   OLD: return val.lower() in (\"0\", \"false\", \"no\")\n",
      "   NEW: return val_demo.lower() in (\"0\", \"false\", \"no\")\n",
      "\n",
      "7. MODIFICATION\n",
      "   Line 138→138\n",
      "   OLD: # Execute the generator to the sentinel value. This ensures the context is\n",
      "   NEW: # Execute the generator to the sentinel val_demoue. This ensures the context is\n",
      "\n",
      "8. MODIFICATION\n",
      "   Line 148→148\n",
      "   OLD: views do not have to return response objects but can return a value that\n",
      "   NEW: views do not have to return response objects but can return a val_demoue that\n",
      "\n",
      "9. MODIFICATION\n",
      "   Line 171→171\n",
      "   OLD: The other use case of this function is to force the return value of a\n",
      "   NEW: The other use case of this function is to force the return val_demoue of a\n",
      "\n",
      "10. MODIFICATION\n",
      "   Line 202→202\n",
      "   OLD: **values: t.Any,\n",
      "   NEW: **val_demoues: t.Any,\n",
      "\n",
      "11. MODIFICATION\n",
      "   Line 204→204\n",
      "   OLD: \"\"\"Generate a URL to the given endpoint with the given values.\n",
      "   NEW: \"\"\"Generate a URL to the given endpoint with the given val_demoues.\n",
      "\n",
      "12. MODIFICATION\n",
      "   Line 222→222\n",
      "   OLD: :param values: Values to use for the variable parts of the URL rule.\n",
      "   NEW: :param val_demoues: val_demoues to use for the variable parts of the URL rule.\n",
      "\n",
      "13. MODIFICATION\n",
      "   Line 245→245\n",
      "   OLD: **values,\n",
      "   NEW: **val_demoues,\n",
      "\n",
      "14. MODIFICATION\n",
      "   Line 327→327\n",
      "   OLD: :param category: the category for the message.  The following values\n",
      "   NEW: :param category: the category for the message.  The following val_demoues\n",
      "\n",
      "15. MODIFICATION\n",
      "   Line 339→339\n",
      "   OLD: # implementations that use external storage for keeping their keys/values.\n",
      "   NEW: # implementations that use external storage for keeping their keys/val_demoues.\n",
      "\n",
      "16. MODIFICATION\n",
      "   Line 358→358\n",
      "   OLD: but when `with_categories` is set to ``True``, the return value will\n",
      "   NEW: but when `with_categories` is set to ``True``, the return val_demoue will\n",
      "\n",
      "17. MODIFICATION\n",
      "   Line 380→380\n",
      "   OLD: :param category_filter: filter of categories to limit return values.  Only\n",
      "   NEW: :param category_filter: filter of categories to limit return val_demoues.  Only\n",
      "\n",
      "18. MODIFICATION\n",
      "   Line 473→473\n",
      "   OLD: :class:`~io.TextIOBase` will raise a :exc:`ValueError` rather\n",
      "   NEW: :class:`~io.TextIOBase` will raise a :exc:`val_demoueError` rather\n",
      "\n",
      "19. MODIFICATION\n",
      "   Line 518→518\n",
      "   OLD: return werkzeug.utils.send_file(  # type: ignore[return-value]\n",
      "   NEW: return werkzeug.utils.send_file(  # type: ignore[return-val_demoue]\n",
      "\n",
      "20. MODIFICATION\n",
      "   Line 558→558\n",
      "   OLD: be a value provided by the client, otherwise it becomes insecure.\n",
      "   NEW: be a val_demoue provided by the client, otherwise it becomes insecure.\n",
      "\n",
      "21. MODIFICATION\n",
      "   Line 572→572\n",
      "   OLD: return werkzeug.utils.send_from_directory(  # type: ignore[return-value]\n",
      "   NEW: return werkzeug.utils.send_from_directory(  # type: ignore[return-val_demoue]\n",
      "\n",
      "22. MODIFICATION\n",
      "   Line 582→582\n",
      "   OLD: Not to be confused with the value returned by :func:`find_package`.\n",
      "   NEW: Not to be confused with the val_demoue returned by :func:`find_package`.\n",
      "\n",
      "23. DELETION\n",
      "   Line 597: - raise ValueError\n",
      "\n",
      "24. MODIFICATION\n",
      "   Line 598→597\n",
      "   OLD: except (ImportError, ValueError):\n",
      "   NEW: raise val_demoueError\n",
      "\n",
      "25. ADDITION\n",
      "   Line 598: + except (ImportError, val_demoueError):\n",
      "\n",
      "\n",
      "Formatted for LLM prompt:\n",
      "1. **Removed** at line 32: `val = os.environ.get(\"FLASK_DEBUG\")`\n",
      "2. **Changed** at line 33: `return bool(val and val.lower() not in {\"0\", \"false\", \"no\"})` → `val_demo = os.environ.get(\"FLASK_DEBUG\")`\n",
      "3. **Added** at line 33: `return bool(val_demo and val_demo.lower() not in {\"0\", \"false\", \"no\"})`\n",
      "4. **Changed** at line 43: `val = os.environ.get(\"FLASK_SKIP_DOTENV\")` → `val_demo = os.environ.get(\"FLASK_SKIP_DOTENV\")`\n",
      "5. **Changed** at line 45: `if not val:` → `if not val_demo:`\n",
      "6. **Changed** at line 48: `return val.lower() in (\"0\", \"false\", \"no\")` → `return val_demo.lower() in (\"0\", \"false\", \"no\")`\n",
      "7. **Changed** at line 138: `# Execute the generator to the sentinel value. This ensures the context is` → `# Execute the generator to the sentinel val_demoue. This ensures the context is`\n",
      "8. **Changed** at line 148: `views do not have to return response objects but can return a value that` → `views do not have to return response objects but can return a val_demoue that`\n",
      "9. **Changed** at line 171: `The other use case of this function is to force the return value of a` → `The other use case of this function is to force the return val_demoue of a`\n",
      "10. **Changed** at line 202: `**values: t.Any,` → `**val_demoues: t.Any,`\n",
      "11. **Changed** at line 204: `\"\"\"Generate a URL to the given endpoint with the given values.` → `\"\"\"Generate a URL to the given endpoint with the given val_demoues.`\n",
      "12. **Changed** at line 222: `:param values: Values to use for the variable parts of the URL rule.` → `:param val_demoues: val_demoues to use for the variable parts of the URL rule.`\n",
      "13. **Changed** at line 245: `**values,` → `**val_demoues,`\n",
      "14. **Changed** at line 327: `:param category: the category for the message.  The following values` → `:param category: the category for the message.  The following val_demoues`\n",
      "15. **Changed** at line 339: `# implementations that use external storage for keeping their keys/values.` → `# implementations that use external storage for keeping their keys/val_demoues.`\n",
      "16. **Changed** at line 358: `but when `with_categories` is set to ``True``, the return value will` → `but when `with_categories` is set to ``True``, the return val_demoue will`\n",
      "17. **Changed** at line 380: `:param category_filter: filter of categories to limit return values.  Only` → `:param category_filter: filter of categories to limit return val_demoues.  Only`\n",
      "18. **Changed** at line 473: `:class:`~io.TextIOBase` will raise a :exc:`ValueError` rather` → `:class:`~io.TextIOBase` will raise a :exc:`val_demoueError` rather`\n",
      "19. **Changed** at line 518: `return werkzeug.utils.send_file(  # type: ignore[return-value]` → `return werkzeug.utils.send_file(  # type: ignore[return-val_demoue]`\n",
      "20. **Changed** at line 558: `be a value provided by the client, otherwise it becomes insecure.` → `be a val_demoue provided by the client, otherwise it becomes insecure.`\n",
      "21. **Changed** at line 572: `return werkzeug.utils.send_from_directory(  # type: ignore[return-value]` → `return werkzeug.utils.send_from_directory(  # type: ignore[return-val_demoue]`\n",
      "22. **Changed** at line 582: `Not to be confused with the value returned by :func:`find_package`.` → `Not to be confused with the val_demoue returned by :func:`find_package`.`\n",
      "23. **Removed** at line 597: `raise ValueError`\n",
      "24. **Changed** at line 598: `except (ImportError, ValueError):` → `raise val_demoueError`\n",
      "25. **Added** at line 598: `except (ImportError, val_demoueError):`\n"
     ]
    }
   ],
   "source": [
    "# Test atomic change detection on a sample file\n",
    "test_file = \"src/flask/helpers.py\"\n",
    "\n",
    "if test_file in get_changed_files(base_branch, current_branch, REPO_PATH):\n",
    "    print(f\"Testing atomic change detection for: {test_file}\\n\")\n",
    "    \n",
    "    # Get the diff\n",
    "    test_diff = get_file_diff(base_branch, current_branch, test_file, REPO_PATH)\n",
    "    \n",
    "    # Parse atomic changes\n",
    "    atomic_changes = parse_diff_hunks(test_diff)\n",
    "    atomic_changes = detect_modifications(atomic_changes)\n",
    "    \n",
    "    print(f\"Detected {len(atomic_changes)} atomic changes:\\n\")\n",
    "    \n",
    "    for i, change in enumerate(atomic_changes, 1):\n",
    "        print(f\"{i}. {change.change_type.upper()}\")\n",
    "        if change.change_type == 'addition':\n",
    "            print(f\"   Line {change.new_line}: + {change.new_content}\")\n",
    "        elif change.change_type == 'deletion':\n",
    "            print(f\"   Line {change.old_line}: - {change.old_content}\")\n",
    "        elif change.change_type == 'modification':\n",
    "            print(f\"   Line {change.old_line}→{change.new_line}\")\n",
    "            print(f\"   OLD: {change.old_content}\")\n",
    "            print(f\"   NEW: {change.new_content}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nFormatted for LLM prompt:\")\n",
    "    print(format_atomic_changes(atomic_changes))\n",
    "else:\n",
    "    print(f\"{test_file} not found in changed files. Choose another file to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6fab6",
   "metadata": {},
   "source": [
    "## 11. Retry Failed File Summaries (On-Demand)\n",
    "\n",
    "If some files failed to summarize due to LLM timeouts, you can retry them individually with a longer timeout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a06f26",
   "metadata": {},
   "source": [
    "### Regenerate Overall PR Summary After Retry\n",
    "\n",
    "After retrying failed files, you should regenerate the overall PR summary so it includes all newly summarized files. Use the helper below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23abf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_overall_summary(result: dict, llm_timeout: int = 200, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Regenerate the overall PR summary using the latest set of successful file summaries.\n",
    "    This should be called after retrying failed files so the overall summary is up-to-date.\n",
    "    \"\"\"\n",
    "    failed_files = result.get('failed_files', [])\n",
    "    successful_summaries = {\n",
    "        file: summary for file, summary in result['file_summaries'].items()\n",
    "        if file not in failed_files\n",
    "    }\n",
    "    if not successful_summaries:\n",
    "        overall_summary = \"No files could be summarized successfully.\"\n",
    "    else:\n",
    "        summary_list = [f\"{file}: {summary}\" for file, summary in successful_summaries.items()]\n",
    "        overall_prompt = create_overall_summary_prompt(\n",
    "            result['base_branch'],\n",
    "            result['current_branch'],\n",
    "            result['commits'],\n",
    "            result['changed_files'],\n",
    "            summary_list\n",
    "        )\n",
    "        overall_summary = call_ollama(overall_prompt, timeout=llm_timeout)\n",
    "        if not overall_summary:\n",
    "            overall_summary = \"Error generating overall summary due to LLM timeout or error.\"\n",
    "    result['overall_summary'] = overall_summary\n",
    "    if verbose:\n",
    "        print(\"✓ Overall PR summary regenerated.\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8c2e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 file(s) failed to summarize:\n",
      "\n",
      "  1. src/flask/helpers.py\n",
      "\n",
      "To retry a specific file, use:\n",
      "  result = summarize_failed_file(result, '<file_path>', timeout=600)\n",
      "\n",
      " Retrying summarization for: src/flask/helpers.py\n",
      "   Timeout: 600 seconds\n",
      "   → 25 atomic changes detected\n",
      "   ✓ Summary generated successfully!\n",
      "   Coverage: 96.0% (24/25 changes)\n",
      "   ✓ Successfully updated summary for src/flask/helpers.py\n",
      "   Remaining failed files: 0\n",
      "PR SUMMARY\n",
      "\n",
      "Branch: test/pr-summary-demo → main\n",
      "Commits: 4\n",
      "Changed files: 6\n",
      "\n",
      "Recent Commits:\n",
      "   62a77913 - Add debug print and TODO in views.py; update README for PR summarization demo\n",
      "   06e61ab5 - Rename variable in helpers.py for PR summarization demo\n",
      "   1336cc19 - Add test comment to app.py for PR summarization demo\n",
      "   ad68a126 - drop experimental 3.13t test env\n",
      "✓ Successfully Summarized Files (4):\n",
      "\n",
      "🔹 README.md\n",
      "    1/1 changes covered (100.0%)\n",
      "   The README.md file has been modified to include a new line with the text \"Test change for PR summarization demo.\" at line 55, which adds a new line to the file.\n",
      "\n",
      "🔹 src/flask/app.py\n",
      "    1/1 changes covered (100.0%)\n",
      "   This file contains one atomic change: the addition of a comment at line 1. The comment reads \"#Test: Added comment for PR summarization demo\". This comment was added to the file as part of a pull request and serves as a demonstration of how changes can be summarized using Git's diff output.\n",
      "\n",
      "🔹 src/flask/helpers.py\n",
      "    24/25 changes covered (96.0%)\n",
      "   This file contains 25 atomic changes that modify the `src/flask/helpers.py` file in the Flask framework. The changes include removing lines 32 and 43, changing lines 33 and 45 to add a new variable `val_demo`, changing lines 138, 148, and 202 to change the type of some variables, adding a new line at line 204, removing line 597, and changing lines 598 and 599. These changes affect the behavior of various functions in the file, including `get_debug_flag`, `get_load_dotenv`, `make_response`,\n",
      "\n",
      "🔹 src/flask/views.py\n",
      "    1/1 changes covered (100.0%)\n",
      "   The code change in `src/flask/views.py` adds a new line of code at line 104, which prints the message \"Debug: Entered some_view\" to the console when the view function is entered. This change was made by adding the print statement and setting the `init_every_request` class attribute to `True`.\n",
      " Validation Summary:\n",
      "   Total atomic changes tracked: 28\n",
      "   Changes mentioned in summaries: 27\n",
      "   Average coverage: 99.0%\n",
      " Overall PR Summary:\n",
      "This pull request modifies several files in the Flask project, including the README file, app.py, and views.py. The main change is the addition of a new line to the README file, which includes the text \"Test change for PR summarization demo.\" Additionally, a comment has been added to the app.py file, which reads \"#Test: Added comment for PR summarization demo\". Finally, a print statement has been added to the views.py file at line 104, which prints the message \"Debug: Entered some_view\" when the view function is entered. Overall, this pull request serves as a demonstration of how changes can be summarized using Git's diff\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any failed files\n",
    "list_failed_files(result)\n",
    "\n",
    "# Example: Retry a specific failed file with longer timeout\n",
    "# Uncomment and adjust the file path as needed:\n",
    "result = summarize_failed_file(result, \"src/flask/helpers.py\", timeout=600, verbose=True)\n",
    "\n",
    "# After retry, you can re-display and re-export:\n",
    "display_pr_summary(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d618e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Overall PR summary regenerated.\n"
     ]
    }
   ],
   "source": [
    "result = regenerate_overall_summary(result, llm_timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e5b7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PR summary exported to ../pr_summary-demo03.md\n"
     ]
    }
   ],
   "source": [
    "export_summary_to_markdown(result, output_path=\"../pr_summary-demo03.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57353eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
