{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a86fd3",
   "metadata": {},
   "source": [
    "## API Key Configuration\n",
    "\n",
    "**Security Note:** Never hardcode your API key in notebooks or commit it to version control.\n",
    "\n",
    "### Recommended Setup Methods:\n",
    "1. **Environment Variable (Best)**: Set `LLM_API_KEY` in your system environment\n",
    "2. **Config File**: Store in a `.env` file (add to `.gitignore`)\n",
    "3. **User Input**: Enter manually when prompted (not saved)\n",
    "\n",
    "### API Provider Setup:\n",
    "You can adapt it for:\n",
    "- Together AI: https://api.together.xyz\n",
    "- Replicate: https://replicate.com\n",
    "- Hugging Face: https://huggingface.co/inference-api\n",
    "- Custom OpenAI-compatible endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c64136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded from .env file\n",
      "\n",
      "  API key loaded successfully. It will NOT be displayed in any outputs.\n",
      "Key length: 56 characters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from typing import Optional\n",
    "\n",
    "def load_api_key() -> str:\n",
    "    \"\"\"\n",
    "    Securely load LLM API key from environment variable or user input.\n",
    "    The key is never printed or displayed in notebook outputs.\n",
    "    \"\"\"\n",
    "    # Try to load from environment variable first\n",
    "    api_key = os.environ.get('LLM_API_KEY')\n",
    "    \n",
    "    if api_key:\n",
    "        print(\"✓ API key loaded from environment variable\")\n",
    "        return api_key\n",
    "    \n",
    "    # Try to load from .env file\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        api_key = os.environ.get('LLM_API_KEY')\n",
    "        if api_key:\n",
    "            print(\"✓ API key loaded from .env file\")\n",
    "            return api_key\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Prompt user for API key (not echoed to terminal)\n",
    "    print(\"API key not found in environment.\")\n",
    "    api_key = getpass.getpass(\"Enter your LLM API key: \")\n",
    "    \n",
    "    if not api_key or api_key.strip() == \"\":\n",
    "        raise ValueError(\"API key is required to use this notebook\")\n",
    "    \n",
    "    print(\"✓ API key entered manually (not saved)\")\n",
    "    return api_key.strip()\n",
    "\n",
    "# Load API key securely\n",
    "API_KEY = load_api_key()\n",
    "print(\"\\n  API key loaded successfully. It will NOT be displayed in any outputs.\")\n",
    "print(\"Key length:\", len(API_KEY), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9082be5",
   "metadata": {},
   "source": [
    "## 1. Git Data Extraction\n",
    "\n",
    "Extract all relevant PR information: branches, commits, changed files, and diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1640f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Git utility functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from typing import List, Dict\n",
    "\n",
    "# Get current git branch\n",
    "def get_current_branch(repo_path: str = \".\") -> str:\n",
    "    \"\"\"Get the name of the current git branch.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    return result.stdout.strip()\n",
    "\n",
    "# Get base branch (default: main)\n",
    "def get_base_branch(repo_path: str = \".\", default: str = \"main\") -> str:\n",
    "    \"\"\"\n",
    "    Get the base branch for comparison. \n",
    "    Tries to detect main/master, falls back to provided default.\n",
    "    \"\"\"\n",
    "    # Check if main exists\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"--verify\", \"main\"],\n",
    "        capture_output=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        return \"main\"\n",
    "    \n",
    "    # Check if master exists\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"rev-parse\", \"--verify\", \"master\"],\n",
    "        capture_output=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        return \"master\"\n",
    "    \n",
    "    return default\n",
    "\n",
    "# Get list of changed files between base and current branch\n",
    "def get_changed_files(base: str, current: str, repo_path: str = \".\") -> List[str]:\n",
    "    \"\"\"List all files changed between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", \"--name-only\", f\"{base}...{current}\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    files = result.stdout.strip().splitlines()\n",
    "    return [f for f in files if f]  # Filter empty strings\n",
    "\n",
    "# Get commit messages between base and current branch\n",
    "def get_commit_messages(base: str, current: str, repo_path: str = \".\") -> List[str]:\n",
    "    \"\"\"Get all commit messages between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"log\", f\"{base}..{current}\", \"--pretty=format:%h - %s\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    messages = result.stdout.strip().splitlines()\n",
    "    return [m for m in messages if m]\n",
    "\n",
    "# Get diff between base and current branch\n",
    "def get_diff(base: str, current: str, repo_path: str = \".\") -> str:\n",
    "    \"\"\"Get the full diff between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", f\"{base}...{current}\"], \n",
    "        capture_output=True, \n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "# Get diff for a specific file\n",
    "def get_file_diff(base: str, current: str, file_path: str, repo_path: str = \".\") -> str:\n",
    "    \"\"\"Get the diff for a specific file between two branches.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"diff\", f\"{base}...{current}\", \"--\", file_path],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=repo_path\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "print(\"✓ Git utility functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b5326",
   "metadata": {},
   "source": [
    "## 2. Test Git Extraction\n",
    "\n",
    "Verify that we can extract all PR data from the current repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcb1352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current branch: test/pr-summary-demo\n",
      "Base branch: main\n",
      "\n",
      "Changed files (6):\n",
      "  1. .github/workflows/tests.yaml\n",
      "  2. README.md\n",
      "  3. pyproject.toml\n",
      "  4. src/flask/app.py\n",
      "  5. src/flask/helpers.py\n",
      "  6. src/flask/views.py\n",
      "\n",
      "Commit messages (4):\n",
      "  - 62a77913 - Add debug print and TODO in views.py; update README for PR summarization demo\n",
      "  - 06e61ab5 - Rename variable in helpers.py for PR summarization demo\n",
      "  - 1336cc19 - Add test comment to app.py for PR summarization demo\n",
      "  - ad68a126 - drop experimental 3.13t test env\n",
      "\n",
      "Total diff size: 239 lines (9727 characters)\n",
      "Sample diff (first 500 chars):\n",
      "diff --git a/.github/workflows/tests.yaml b/.github/workflows/tests.yaml\n",
      "index 892573d8..347e90d5 100644\n",
      "--- a/.github/workflows/tests.yaml\n",
      "+++ b/.github/workflows/tests.yaml\n",
      "@@ -18,7 +18,6 @@ jobs:\n",
      "           - {name: Windows, python: '3.14', os: windows-latest}\n",
      "           - {name: Mac, python: '3.14', os: macos-latest}\n",
      "           - {python: '3.13'}\n",
      "-          - {python: '3.13t'}\n",
      "           - {python: '3.12'}\n",
      "           - {python: '3.11'}\n",
      "           - {python: '3.10'}\n",
      "diff --git a/README.md b/R...\n"
     ]
    }
   ],
   "source": [
    "# Configure repository path (adjust to your repo location)\n",
    "REPO_PATH = \"../flask\"\n",
    "\n",
    "# Extract PR data\n",
    "current_branch = get_current_branch(REPO_PATH)\n",
    "base_branch = get_base_branch(REPO_PATH)\n",
    "\n",
    "print(f\"Current branch: {current_branch}\")\n",
    "print(f\"Base branch: {base_branch}\")\n",
    "\n",
    "# Get changed files\n",
    "changed_files = get_changed_files(base_branch, current_branch, REPO_PATH)\n",
    "print(f\"\\nChanged files ({len(changed_files)}):\")\n",
    "for i, file in enumerate(changed_files[:10], 1):  # Show first 10\n",
    "    print(f\"  {i}. {file}\")\n",
    "if len(changed_files) > 10:\n",
    "    print(f\"  ... and {len(changed_files) - 10} more\")\n",
    "\n",
    "# Get commit messages\n",
    "commits = get_commit_messages(base_branch, current_branch, REPO_PATH)\n",
    "print(f\"\\nCommit messages ({len(commits)}):\")\n",
    "for commit in commits[:5]:  # Show first 5\n",
    "    print(f\"  - {commit}\")\n",
    "if len(commits) > 5:\n",
    "    print(f\"  ... and {len(commits) - 5} more\")\n",
    "\n",
    "# Get diff size\n",
    "full_diff = get_diff(base_branch, current_branch, REPO_PATH)\n",
    "diff_lines = len(full_diff.splitlines())\n",
    "print(f\"\\nTotal diff size: {diff_lines} lines ({len(full_diff)} characters)\")\n",
    "print(f\"Sample diff (first 500 chars):\\n{full_diff[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ae3fa",
   "metadata": {},
   "source": [
    "## 3. Diff Chunking Strategy\n",
    "\n",
    "For large diffs, we need to chunk them intelligently to avoid overwhelming the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86403f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chunking utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def chunk_diff_by_file(base: str, current: str, changed_files: List[str], repo_path: str = \".\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Split the diff into per-file chunks for manageable summarization.\n",
    "    Returns a dict mapping file paths to their diffs.\n",
    "    \"\"\"\n",
    "    file_diffs = {}\n",
    "    for file_path in changed_files:\n",
    "        diff = get_file_diff(base, current, file_path, repo_path)\n",
    "        if diff.strip():  # Only include files with actual changes\n",
    "            file_diffs[file_path] = diff\n",
    "    return file_diffs\n",
    "\n",
    "def truncate_large_diff(diff: str, max_lines: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Truncate very large diffs to focus on beginning and end.\n",
    "    Useful for summarization when full context isn't needed.\n",
    "    \"\"\"\n",
    "    lines = diff.splitlines()\n",
    "    if len(lines) <= max_lines:\n",
    "        return diff\n",
    "    \n",
    "    # Take first and last portions\n",
    "    half = max_lines // 2\n",
    "    truncated = lines[:half] + [\"\\n... [truncated middle section] ...\\n\"] + lines[-half:]\n",
    "    return \"\\n\".join(truncated)\n",
    "\n",
    "def should_summarize_file(file_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a file should be included in summarization.\n",
    "    Exclude generated files, lock files, config files, etc.\n",
    "    \"\"\"\n",
    "    exclude_patterns = [\n",
    "        '.github/',\n",
    "        'pyproject.toml',\n",
    "        'package-lock.json',\n",
    "        'yarn.lock',\n",
    "        'poetry.lock',\n",
    "        '.min.js',\n",
    "        '.min.css',\n",
    "        '__pycache__',\n",
    "        '.pyc',\n",
    "        '.yml',\n",
    "        '.yaml',\n",
    "        'requirements.txt',\n",
    "        'setup.py',  \n",
    "        'setup.cfg',           \n",
    "        '.gitignore',        \n",
    "        'LICENSE',    \n",
    "        'MANIFEST.in'    \n",
    "    ]\n",
    "    for pattern in exclude_patterns:\n",
    "        if pattern in file_path:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(\"✓ Chunking utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90679aab",
   "metadata": {},
   "source": [
    "## 3.5 Atomic Diff Parsing\n",
    "\n",
    "Parse git diffs into atomic changes for precise tracking and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02af05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Atomic diff parser loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class AtomicChange:\n",
    "    \"\"\"Represents a single atomic change in a diff.\"\"\"\n",
    "    change_type: str  # 'addition', 'deletion', 'modification'\n",
    "    line_number: int\n",
    "    old_line: Optional[int]\n",
    "    new_line: Optional[int]\n",
    "    old_content: Optional[str]\n",
    "    new_content: Optional[str]\n",
    "    context: str  # Surrounding code context\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.change_type == 'addition':\n",
    "            return f\"Line {self.new_line}: + {self.new_content}\"\n",
    "        elif self.change_type == 'deletion':\n",
    "            return f\"Line {self.old_line}: - {self.old_content}\"\n",
    "        else:\n",
    "            return f\"Line {self.old_line}->{self.new_line}: {self.old_content} → {self.new_content}\"\n",
    "\n",
    "def parse_diff_hunks(diff: str) -> List[AtomicChange]:\n",
    "    \"\"\"Parse a git diff into atomic changes with line numbers and context.\"\"\"\n",
    "    changes = []\n",
    "    lines = diff.splitlines()\n",
    "    \n",
    "    old_line_num = 0\n",
    "    new_line_num = 0\n",
    "    context_buffer = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('@@'):\n",
    "            match = re.match(r'@@ -(\\d+),?\\d* \\+(\\d+),?\\d* @@', line)\n",
    "            if match:\n",
    "                old_line_num = int(match.group(1))\n",
    "                new_line_num = int(match.group(2))\n",
    "                context_buffer = []\n",
    "            continue\n",
    "        \n",
    "        if line.startswith('diff --git') or line.startswith('index') or \\\n",
    "           line.startswith('---') or line.startswith('+++'):\n",
    "            continue\n",
    "        \n",
    "        if line.startswith(' '):\n",
    "            context_buffer.append(line[1:])\n",
    "            if len(context_buffer) > 2:\n",
    "                context_buffer.pop(0)\n",
    "            old_line_num += 1\n",
    "            new_line_num += 1\n",
    "            \n",
    "        elif line.startswith('+'):\n",
    "            content = line[1:].strip()\n",
    "            if content:\n",
    "                context = '\\n'.join(context_buffer[-2:]) if context_buffer else \"\"\n",
    "                changes.append(AtomicChange(\n",
    "                    change_type='addition',\n",
    "                    line_number=new_line_num,\n",
    "                    old_line=None,\n",
    "                    new_line=new_line_num,\n",
    "                    old_content=None,\n",
    "                    new_content=content,\n",
    "                    context=context\n",
    "                ))\n",
    "            new_line_num += 1\n",
    "            \n",
    "        elif line.startswith('-'):\n",
    "            content = line[1:].strip()\n",
    "            if content:\n",
    "                context = '\\n'.join(context_buffer[-2:]) if context_buffer else \"\"\n",
    "                changes.append(AtomicChange(\n",
    "                    change_type='deletion',\n",
    "                    line_number=old_line_num,\n",
    "                    old_line=old_line_num,\n",
    "                    new_line=None,\n",
    "                    old_content=content,\n",
    "                    new_content=None,\n",
    "                    context=context\n",
    "                ))\n",
    "            old_line_num += 1\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def detect_modifications(changes: List[AtomicChange]) -> List[AtomicChange]:\n",
    "    \"\"\"Post-process changes to detect modifications (deletion + addition pairs).\"\"\"\n",
    "    if not changes:\n",
    "        return changes\n",
    "    \n",
    "    modified_changes = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(changes):\n",
    "        current = changes[i]\n",
    "        \n",
    "        if (current.change_type == 'deletion' and \n",
    "            i + 1 < len(changes) and \n",
    "            changes[i + 1].change_type == 'addition' and\n",
    "            abs(current.line_number - changes[i + 1].line_number) <= 2):\n",
    "            \n",
    "            next_change = changes[i + 1]\n",
    "            modified_changes.append(AtomicChange(\n",
    "                change_type='modification',\n",
    "                line_number=current.line_number,\n",
    "                old_line=current.old_line,\n",
    "                new_line=next_change.new_line,\n",
    "                old_content=current.old_content,\n",
    "                new_content=next_change.new_content,\n",
    "                context=current.context\n",
    "            ))\n",
    "            i += 2\n",
    "        else:\n",
    "            modified_changes.append(current)\n",
    "            i += 1\n",
    "    \n",
    "    return modified_changes\n",
    "\n",
    "def format_atomic_changes(changes: List[AtomicChange]) -> str:\n",
    "    \"\"\"Format atomic changes into a clear, enumerated list for LLM prompts.\"\"\"\n",
    "    if not changes:\n",
    "        return \"No atomic changes detected.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for idx, change in enumerate(changes, 1):\n",
    "        if change.change_type == 'addition':\n",
    "            formatted.append(f\"{idx}. **Added** at line {change.new_line}: `{change.new_content}`\")\n",
    "        elif change.change_type == 'deletion':\n",
    "            formatted.append(f\"{idx}. **Removed** at line {change.old_line}: `{change.old_content}`\")\n",
    "        elif change.change_type == 'modification':\n",
    "            formatted.append(\n",
    "                f\"{idx}. **Changed** at line {change.old_line}: \"\n",
    "                f\"`{change.old_content}` → `{change.new_content}`\"\n",
    "            )\n",
    "    \n",
    "    return '\\n'.join(formatted)\n",
    "\n",
    "print(\"✓ Atomic diff parser loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f990ef",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering\n",
    "\n",
    "Create effective prompts for the LLM to generate high-quality summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326050e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt templates created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_file_summary_prompt(file_path: str, diff: str, max_diff_lines: int = 150) -> str:\n",
    "    \"\"\"Create a prompt for summarizing a single file's changes.\"\"\"\n",
    "    atomic_changes = parse_diff_hunks(diff)\n",
    "    atomic_changes = detect_modifications(atomic_changes)\n",
    "    \n",
    "    changes_list = format_atomic_changes(atomic_changes)\n",
    "    change_count = len(atomic_changes)\n",
    "    truncated_diff = truncate_large_diff(diff, max_diff_lines)\n",
    "    \n",
    "    prompt = f\"\"\"Summarize the code changes for this file. You must mention ALL {change_count} changes listed below.\n",
    "\n",
    "File: {file_path}\n",
    "\n",
    "Atomic Changes ({change_count} total):\n",
    "{changes_list}\n",
    "\n",
    "Full Diff Context:\n",
    "```\n",
    "{truncated_diff}\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "- Describe ALL {change_count} atomic changes listed above\n",
    "- Be specific: mention variable names, function names, line additions/deletions\n",
    "- **Write a single concise paragraph (1-2 sentences), not a bullet list**\n",
    "- Do not infer or hallucinate changes not shown above\n",
    "\n",
    "Summary (concise paragraph):\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_overall_summary_prompt(\n",
    "    base_branch: str, \n",
    "    current_branch: str, \n",
    "    commits: list, \n",
    "    changed_files: list,\n",
    "    file_summaries: list\n",
    ") -> str:\n",
    "    \"\"\"Create a prompt for generating an overall PR summary from file-level summaries.\"\"\"\n",
    "    commits_text = \"\\n\".join(f\"  - {commit}\" for commit in commits[:10])\n",
    "    if len(commits) > 10:\n",
    "        commits_text += f\"\\n  ... and {len(commits) - 10} more commits\"\n",
    "    \n",
    "    files_text = \"\\n\".join(f\"  - {file}\" for file in changed_files[:15])\n",
    "    if len(changed_files) > 15:\n",
    "        files_text += f\"\\n  ... and {len(changed_files) - 15} more files\"\n",
    "    \n",
    "    summaries_text = \"\\n\\n\".join(f\"{i+1}. {summary}\" for i, summary in enumerate(file_summaries))\n",
    "    \n",
    "    prompt = f\"\"\"Summarize this pull request based only on the information below. Be concise (2-3 sentences total).\n",
    "\n",
    "Branch: {current_branch} → {base_branch}\n",
    "\n",
    "Commits: {len(commits)}\n",
    "Changed files: {len(changed_files)}\n",
    "\n",
    "File summaries:\n",
    "{summaries_text}\n",
    "\n",
    "Provide a brief PR summary covering: purpose, main changes, and impact. Keep it under 3 sentences total.\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"✓ Prompt templates created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35b424",
   "metadata": {},
   "source": [
    "## 4.5 Summary Validation\n",
    "\n",
    "Verify that LLM summaries cover all atomic changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5444a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Summary validation utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def validate_summary_coverage(\n",
    "    summary: str, \n",
    "    atomic_changes: List[AtomicChange],\n",
    "    file_path: str\n",
    ") -> Tuple[bool, List[AtomicChange], Dict[str, any]]:\n",
    "    \"\"\"Validate that the LLM summary mentions all atomic changes.\"\"\"\n",
    "    if not summary or not atomic_changes:\n",
    "        return True, [], {\"total\": 0, \"mentioned\": 0, \"coverage\": 100.0}\n",
    "    \n",
    "    summary_lower = summary.lower()\n",
    "    missing_changes = []\n",
    "    mentioned_count = 0\n",
    "    \n",
    "    for change in atomic_changes:\n",
    "        is_mentioned = False\n",
    "        \n",
    "        if change.change_type == 'addition' and change.new_content:\n",
    "            content_keywords = extract_keywords(change.new_content)\n",
    "            if any(kw in summary_lower for kw in content_keywords):\n",
    "                is_mentioned = True\n",
    "                \n",
    "        elif change.change_type == 'deletion' and change.old_content:\n",
    "            content_keywords = extract_keywords(change.old_content)\n",
    "            if any(kw in summary_lower for kw in content_keywords) or \\\n",
    "               'remov' in summary_lower or 'delet' in summary_lower:\n",
    "                is_mentioned = True\n",
    "                \n",
    "        elif change.change_type == 'modification':\n",
    "            old_keywords = extract_keywords(change.old_content) if change.old_content else []\n",
    "            new_keywords = extract_keywords(change.new_content) if change.new_content else []\n",
    "            if any(kw in summary_lower for kw in old_keywords + new_keywords) or \\\n",
    "               'chang' in summary_lower or 'modif' in summary_lower or 'renam' in summary_lower:\n",
    "                is_mentioned = True\n",
    "        \n",
    "        if is_mentioned:\n",
    "            mentioned_count += 1\n",
    "        else:\n",
    "            missing_changes.append(change)\n",
    "    \n",
    "    total = len(atomic_changes)\n",
    "    coverage = (mentioned_count / total * 100) if total > 0 else 100.0\n",
    "    \n",
    "    metrics = {\n",
    "        \"file\": file_path,\n",
    "        \"total_changes\": total,\n",
    "        \"mentioned_changes\": mentioned_count,\n",
    "        \"missing_changes\": len(missing_changes),\n",
    "        \"coverage_percent\": coverage\n",
    "    }\n",
    "    \n",
    "    is_complete = coverage >= 80.0\n",
    "    return is_complete, missing_changes, metrics\n",
    "\n",
    "def extract_keywords(text: str) -> List[str]:\n",
    "    \"\"\"Extract meaningful keywords from code text for validation.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b[a-z_][a-z0-9_]*\\b', text)\n",
    "    \n",
    "    common_keywords = {'if', 'else', 'for', 'while', 'return', 'def', 'class', \n",
    "                      'import', 'from', 'as', 'in', 'is', 'and', 'or', 'not', 'the', 'a', 'an'}\n",
    "    keywords = [w for w in words if len(w) > 2 and w not in common_keywords]\n",
    "    \n",
    "    return keywords[:5]\n",
    "\n",
    "def create_reprompt_for_missing_changes(\n",
    "    file_path: str,\n",
    "    original_summary: str,\n",
    "    missing_changes: List[AtomicChange]\n",
    ") -> str:\n",
    "    \"\"\"Create a focused re-prompt for changes that were not covered in the initial summary.\"\"\"\n",
    "    changes_list = format_atomic_changes(missing_changes)\n",
    "    \n",
    "    prompt = f\"\"\"Your previous summary for {file_path} missed some changes. Please describe these specific changes:\n",
    "\n",
    "Missing Changes:\n",
    "{changes_list}\n",
    "\n",
    "Previous Summary:\n",
    "{original_summary}\n",
    "\n",
    "Provide a brief description (1 sentence) of the missing changes above:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"✓ Summary validation utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a59b14",
   "metadata": {},
   "source": [
    "## 5. API-Based LLM Integration\n",
    "\n",
    "Connect to the LLM API and send prompts for summarization.\n",
    "\n",
    "**Key Differences from Local Ollama:**\n",
    "- Uses HTTP API with authentication\n",
    "- Supports multiple providers and models\n",
    "- Has robust error handling and retry logic\n",
    "- API key is never exposed in outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d931d998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API connection...\n",
      "✓ API connection successful!\n",
      "  Model: llama-3.3-70b-versatile\n",
      "  Response received: 2 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "# API Configuration\n",
    "API_URL = \"https://api.groq.com/openai/v1/chat/completions\"  # Groq's AI endpoint\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"  # Default model\n",
    "\n",
    "def call_llm_api(\n",
    "    prompt: str, \n",
    "    model: str = MODEL_NAME,\n",
    "    temperature: float = 0.3, \n",
    "    timeout: int = 200,\n",
    "    max_retries: int = 3\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Send a prompt to the LLM API and return the generated response.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt for the LLM\n",
    "        model: Model name (default: llama-3.1-70b)\n",
    "        temperature: Sampling temperature (lower = more focused)\n",
    "        timeout: Request timeout in seconds (default: 200)\n",
    "        max_retries: Maximum number of retry attempts on failure\n",
    "    \n",
    "    Returns:\n",
    "        The generated text response, or None if the request fails\n",
    "        \n",
    "    Security: API key is passed in headers but never logged or printed.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",  # API key is secure, never printed\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful code analysis assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": 500,  # Increased for better summaries\n",
    "        \"stop\": None\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            # Extract response from API format\n",
    "            if 'choices' in result and len(result['choices']) > 0:\n",
    "                return result['choices'][0]['message']['content'].strip()\n",
    "            else:\n",
    "                print(f\"  Unexpected API response format\")\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"⏱️  LLM request timed out after {timeout} seconds (attempt {attempt + 1}/{max_retries})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"   Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            status_code = e.response.status_code\n",
    "            \n",
    "            # Handle rate limiting\n",
    "            if status_code == 429:\n",
    "                print(f\" Rate limit hit (attempt {attempt + 1}/{max_retries})\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 5 * (attempt + 1)  # Longer wait for rate limits\n",
    "                    print(f\"   Waiting {wait_time} seconds before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\" Rate limit exceeded, max retries reached\")\n",
    "                    return None\n",
    "            \n",
    "            # Handle authentication errors\n",
    "            elif status_code == 401:\n",
    "                print(f\" Authentication failed: Invalid API key\")\n",
    "                return None\n",
    "            \n",
    "            # Handle other HTTP errors\n",
    "            else:\n",
    "                print(f\" HTTP error {status_code}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    return None\n",
    "                    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\" Request failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def test_api_connection() -> bool:\n",
    "    \"\"\"Test if the LLM API is accessible with the provided API key.\"\"\"\n",
    "    print(\"Testing API connection...\")\n",
    "    \n",
    "    test_prompt = \"Respond with only the word 'OK' if you can read this.\"\n",
    "    \n",
    "    try:\n",
    "        response = call_llm_api(test_prompt, timeout=30, max_retries=1)\n",
    "        \n",
    "        if response:\n",
    "            print(f\"✓ API connection successful!\")\n",
    "            print(f\"  Model: {MODEL_NAME}\")\n",
    "            print(f\"  Response received: {len(response)} characters\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\" API test failed - no response received\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" API test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the API connection\n",
    "test_api_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e45c1",
   "metadata": {},
   "source": [
    "## 6. Complete PR Summarization Pipeline\n",
    "\n",
    "Orchestrate all steps: extract data → chunk → summarize → aggregate with full validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43a1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PR summarization pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "def summarize_pr(\n",
    "    repo_path: str = \".\",\n",
    "    base_branch: str = None,\n",
    "    current_branch: str = None,\n",
    "    max_files_to_summarize: int = 10,\n",
    "    enable_validation: bool = True,\n",
    "    retry_missing: bool = True,\n",
    "    llm_timeout: int = 200,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Complete PR summarization pipeline with atomic change tracking and validation.\n",
    "    Uses API-based LLM instead of local Ollama.\n",
    "    \n",
    "    Args:\n",
    "        repo_path: Path to git repository\n",
    "        base_branch: Base branch for comparison (auto-detected if None)\n",
    "        current_branch: Current branch (auto-detected if None)\n",
    "        max_files_to_summarize: Maximum number of files to process\n",
    "        enable_validation: Whether to validate summaries cover all changes\n",
    "        retry_missing: Whether to re-prompt for missing changes\n",
    "        llm_timeout: Timeout in seconds for LLM API requests\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns a dictionary with PR summary data, metrics, and failed files list.\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\" API-BASED PR SUMMARIZATION PIPELINE\")\n",
    "    \n",
    "    # Step 1: Extract git data\n",
    "    if verbose:\n",
    "        print(\"\\n[1/5] Extracting PR data from git...\")\n",
    "    \n",
    "    if not current_branch:\n",
    "        current_branch = get_current_branch(repo_path)\n",
    "    if not base_branch:\n",
    "        base_branch = get_base_branch(repo_path)\n",
    "    \n",
    "    commits = get_commit_messages(base_branch, current_branch, repo_path)\n",
    "    changed_files = get_changed_files(base_branch, current_branch, repo_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Branch: {current_branch} → {base_branch}\")\n",
    "        print(f\"  Commits: {len(commits)}\")\n",
    "        print(f\"  Changed files: {len(changed_files)}\")\n",
    "    \n",
    "    if not changed_files:\n",
    "        return {\n",
    "            \"base_branch\": base_branch,\n",
    "            \"current_branch\": current_branch,\n",
    "            \"commits\": commits,\n",
    "            \"changed_files\": changed_files,\n",
    "            \"file_summaries\": {},\n",
    "            \"failed_files\": [],\n",
    "            \"file_metrics\": {},\n",
    "            \"overall_summary\": \"No changes detected between branches.\",\n",
    "            \"repo_path\": repo_path\n",
    "        }\n",
    "    \n",
    "    # Step 2: Chunk diffs by file\n",
    "    if verbose:\n",
    "        print(f\"\\n[2/5] Chunking diffs by file...\")\n",
    "    \n",
    "    file_diffs = chunk_diff_by_file(base_branch, current_branch, changed_files, repo_path)\n",
    "    \n",
    "    files_to_summarize = [\n",
    "        f for f in changed_files \n",
    "        if should_summarize_file(f) and f in file_diffs\n",
    "    ][:max_files_to_summarize]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Files to summarize: {len(files_to_summarize)}\")\n",
    "    \n",
    "    # Step 3: Summarize each file with atomic change tracking\n",
    "    if verbose:\n",
    "        print(f\"\\n[3/5] Generating file-level summaries via API...\")\n",
    "    \n",
    "    file_summaries = {}\n",
    "    failed_files = []\n",
    "    file_metrics = {}\n",
    "    \n",
    "    for i, file_path in enumerate(files_to_summarize, 1):\n",
    "        if verbose:\n",
    "            print(f\"  [{i}/{len(files_to_summarize)}] {file_path}...\")\n",
    "        \n",
    "        diff = file_diffs[file_path]\n",
    "        atomic_changes = parse_diff_hunks(diff)\n",
    "        atomic_changes = detect_modifications(atomic_changes)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"      → {len(atomic_changes)} atomic changes detected\")\n",
    "        \n",
    "        prompt = create_file_summary_prompt(file_path, diff)\n",
    "        summary = call_llm_api(prompt, timeout=llm_timeout)\n",
    "        \n",
    "        if not summary:\n",
    "            if verbose:\n",
    "                print(f\"       Failed to generate summary (timeout/error)\")\n",
    "            failed_files.append(file_path)\n",
    "            file_summaries[file_path] = \" Summary could not be generated for this file due to API timeout or error.\"\n",
    "            continue\n",
    "        \n",
    "        # Validate coverage\n",
    "        if enable_validation and atomic_changes:\n",
    "            is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "                summary, atomic_changes, file_path\n",
    "            )\n",
    "            file_metrics[file_path] = metrics\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"      ✓ Coverage: {metrics['coverage_percent']:.1f}% \"\n",
    "                      f\"({metrics['mentioned_changes']}/{metrics['total_changes']} changes)\")\n",
    "            \n",
    "            # Re-prompt for missing changes\n",
    "            if not is_complete and retry_missing and missing_changes:\n",
    "                if verbose:\n",
    "                    print(f\"       Re-prompting for {len(missing_changes)} missing changes...\")\n",
    "                \n",
    "                reprompt = create_reprompt_for_missing_changes(file_path, summary, missing_changes)\n",
    "                additional_summary = call_llm_api(reprompt, timeout=llm_timeout)\n",
    "                \n",
    "                if additional_summary:\n",
    "                    summary = f\"{summary} {additional_summary}\"\n",
    "                    \n",
    "                    is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "                        summary, atomic_changes, file_path\n",
    "                    )\n",
    "                    file_metrics[file_path] = metrics\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"      ✓ Updated coverage: {metrics['coverage_percent']:.1f}%\")\n",
    "        \n",
    "        file_summaries[file_path] = summary\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[4/5] Generating overall PR summary...\")\n",
    "    \n",
    "    # Step 4: Generate overall summary\n",
    "    successful_summaries = {\n",
    "        file: summary for file, summary in file_summaries.items()\n",
    "        if file not in failed_files\n",
    "    }\n",
    "    \n",
    "    if not successful_summaries:\n",
    "        overall_summary = \"No files could be summarized successfully.\"\n",
    "    else:\n",
    "        summary_list = [f\"{file}: {summary}\" for file, summary in successful_summaries.items()]\n",
    "        overall_prompt = create_overall_summary_prompt(\n",
    "            base_branch, \n",
    "            current_branch, \n",
    "            commits, \n",
    "            changed_files,\n",
    "            summary_list\n",
    "        )\n",
    "        overall_summary = call_llm_api(overall_prompt, timeout=llm_timeout)\n",
    "        if not overall_summary:\n",
    "            overall_summary = \"Error generating overall summary due to API timeout or error.\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[5/5] Pipeline complete!\")\n",
    "        print(f\"  ✓ Successfully summarized: {len(successful_summaries)} files\")\n",
    "        if failed_files:\n",
    "            print(f\"   Failed to summarize: {len(failed_files)} files\")\n",
    "        if file_metrics:\n",
    "            avg_coverage = sum(m['coverage_percent'] for m in file_metrics.values()) / len(file_metrics)\n",
    "            print(f\"   Average change coverage: {avg_coverage:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"base_branch\": base_branch,\n",
    "        \"current_branch\": current_branch,\n",
    "        \"commits\": commits,\n",
    "        \"changed_files\": changed_files,\n",
    "        \"file_summaries\": file_summaries,\n",
    "        \"failed_files\": failed_files,\n",
    "        \"file_metrics\": file_metrics,\n",
    "        \"overall_summary\": overall_summary,\n",
    "        \"repo_path\": repo_path\n",
    "    }\n",
    "\n",
    "print(\"✓ PR summarization pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860d9fe",
   "metadata": {},
   "source": [
    "## 6.5 On-Demand Retry for Failed Files\n",
    "\n",
    "Re-summarize specific files that failed due to API timeout/error, with configurable timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738de216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ On-demand retry functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def summarize_failed_file(\n",
    "    result: Dict[str, any],\n",
    "    file_path: str,\n",
    "    timeout: int = 600,\n",
    "    enable_validation: bool = True,\n",
    "    retry_missing: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Retry summarization for a specific file that previously failed.\n",
    "    \n",
    "    Args:\n",
    "        result: The PR summarization result dict from summarize_pr\n",
    "        file_path: The specific file to retry (must be in failed_files list)\n",
    "        timeout: Timeout in seconds for API requests (default: 600 for longer wait)\n",
    "        enable_validation: Whether to validate summary coverage\n",
    "        retry_missing: Whether to re-prompt for missing changes\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Updated result dictionary with new summary for the specified file\n",
    "    \"\"\"\n",
    "    \n",
    "    if file_path not in result.get('failed_files', []):\n",
    "        if verbose:\n",
    "            print(f\" {file_path} is not in the failed files list.\")\n",
    "            print(f\"   Failed files: {result.get('failed_files', [])}\")\n",
    "        return result\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n Retrying summarization for: {file_path}\")\n",
    "        print(f\"   Timeout: {timeout} seconds\")\n",
    "    \n",
    "    repo_path = result['repo_path']\n",
    "    base_branch = result['base_branch']\n",
    "    current_branch = result['current_branch']\n",
    "    \n",
    "    diff = get_file_diff(base_branch, current_branch, file_path, repo_path)\n",
    "    \n",
    "    if not diff.strip():\n",
    "        if verbose:\n",
    "            print(f\"     No diff found for {file_path}\")\n",
    "        return result\n",
    "    \n",
    "    atomic_changes = parse_diff_hunks(diff)\n",
    "    atomic_changes = detect_modifications(atomic_changes)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   → {len(atomic_changes)} atomic changes detected\")\n",
    "    \n",
    "    prompt = create_file_summary_prompt(file_path, diff)\n",
    "    summary = call_llm_api(prompt, timeout=timeout)\n",
    "    \n",
    "    if not summary:\n",
    "        if verbose:\n",
    "            print(f\"    Summary still failed with {timeout}s timeout\")\n",
    "        return result\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ✓ Summary generated successfully!\")\n",
    "    \n",
    "    if enable_validation and atomic_changes:\n",
    "        is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "            summary, atomic_changes, file_path\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Coverage: {metrics['coverage_percent']:.1f}% \"\n",
    "                  f\"({metrics['mentioned_changes']}/{metrics['total_changes']} changes)\")\n",
    "        \n",
    "        if not is_complete and retry_missing and missing_changes:\n",
    "            if verbose:\n",
    "                print(f\"    Re-prompting for {len(missing_changes)} missing changes...\")\n",
    "            \n",
    "            reprompt = create_reprompt_for_missing_changes(file_path, summary, missing_changes)\n",
    "            additional_summary = call_llm_api(reprompt, timeout=timeout)\n",
    "            \n",
    "            if additional_summary:\n",
    "                summary = f\"{summary} {additional_summary}\"\n",
    "                \n",
    "                is_complete, missing_changes, metrics = validate_summary_coverage(\n",
    "                    summary, atomic_changes, file_path\n",
    "                )\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"   ✓ Updated coverage: {metrics['coverage_percent']:.1f}%\")\n",
    "        \n",
    "        result['file_metrics'][file_path] = metrics\n",
    "    \n",
    "    result['file_summaries'][file_path] = summary\n",
    "    result['failed_files'].remove(file_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ✓ Successfully updated summary for {file_path}\")\n",
    "        print(f\"   Remaining failed files: {len(result['failed_files'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def list_failed_files(result: Dict[str, any]):\n",
    "    \"\"\"Display a list of all files that failed to summarize.\"\"\"\n",
    "    failed = result.get('failed_files', [])\n",
    "    \n",
    "    if not failed:\n",
    "        print(\"✓ All files were summarized successfully!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n {len(failed)} file(s) failed to summarize:\\\\n\")\n",
    "    for i, file_path in enumerate(failed, 1):\n",
    "        print(f\"  {i}. {file_path}\")\n",
    "    \n",
    "    print(f\"\\\\nTo retry a specific file, use:\")\n",
    "    print(f\"  result = summarize_failed_file(result, '<file_path>', timeout=600)\")\n",
    "\n",
    "print(\"✓ On-demand retry functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f52a5",
   "metadata": {},
   "source": [
    "## 7. Run PR Summarization\n",
    "\n",
    "Execute the pipeline on the current repository and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc4603d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API-BASED PR SUMMARIZATION PIPELINE\n",
      "\n",
      "[1/5] Extracting PR data from git...\n",
      "  Branch: test/pr-summary-demo → main\n",
      "  Commits: 4\n",
      "  Changed files: 6\n",
      "\n",
      "[2/5] Chunking diffs by file...\n",
      "  Files to summarize: 4\n",
      "\n",
      "[3/5] Generating file-level summaries via API...\n",
      "  [1/4] README.md...\n",
      "      → 1 atomic changes detected\n",
      "      ✓ Coverage: 100.0% (1/1 changes)\n",
      "  [2/4] src/flask/app.py...\n",
      "      → 1 atomic changes detected\n",
      "      ✓ Coverage: 100.0% (1/1 changes)\n",
      "  [3/4] src/flask/helpers.py...\n",
      "      → 25 atomic changes detected\n",
      "      ✓ Coverage: 100.0% (25/25 changes)\n",
      "  [4/4] src/flask/views.py...\n",
      "      → 1 atomic changes detected\n",
      "      ✓ Coverage: 100.0% (1/1 changes)\n",
      "\n",
      "[4/5] Generating overall PR summary...\n",
      "\n",
      "[5/5] Pipeline complete!\n",
      "  ✓ Successfully summarized: 4 files\n",
      "   Average change coverage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Run the API-based PR summarization pipeline\n",
    "result = summarize_pr(\n",
    "    repo_path=REPO_PATH,\n",
    "    max_files_to_summarize=5,  # Limit for testing; increase for full analysis\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e55c",
   "metadata": {},
   "source": [
    "## 8. Display Results\n",
    "\n",
    "Format and display the PR summary in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7014a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PR SUMMARY\n",
      "\n",
      " Branch: test/pr-summary-demo → main\n",
      " Commits: 4\n",
      " Changed files: 6\n",
      "\n",
      " Recent Commits:\n",
      "   • 62a77913 - Add debug print and TODO in views.py; update README for PR summarization demo\n",
      "   • 06e61ab5 - Rename variable in helpers.py for PR summarization demo\n",
      "   • 1336cc19 - Add test comment to app.py for PR summarization demo\n",
      "   • ad68a126 - drop experimental 3.13t test env\n",
      "\n",
      " Successfully Summarized Files (4):\n",
      "\n",
      " README.md\n",
      "    1/1 changes covered (100.0%)\n",
      "   The code changes for the README.md file include the addition of the line \"Test change for PR summarization demo.\" at line 55. This single atomic change adds a new sentence to the end of the file, with no other modifications made to the existing content, resulting in a total of 1 change to the file.\n",
      "\n",
      " src/flask/app.py\n",
      "    1/1 changes covered (100.0%)\n",
      "   The code changes for the file `src/flask/app.py` include the addition of a comment `#Test: Added comment for PR summarization demo` at line 1. This single atomic change adds a new line at the beginning of the file, preceding the existing `from __future__ import annotations` line, with no other modifications made to the file.\n",
      "\n",
      " src/flask/helpers.py\n",
      "    25/25 changes covered (100.0%)\n",
      "   The code changes in the `src/flask/helpers.py` file involve 25 atomic modifications, including the removal of `val = os.environ.get(\"FLASK_DEBUG\")` at line 32 and its replacement with `val_demo = os.environ.get(\"FLASK_DEBUG\")` at line 33, as well as numerous changes to variable names, function parameters, and docstrings, such as replacing `values` with `val_demoues` in the `url_for` function at lines 199 and 204, and changing `return-value` to `return-val_demoue` in the `send_file` and `send_from_directory` functions at lines 515 and 569, with other changes including the removal of `raise ValueError` at line 597 and its replacement with `raise val_demoueError` at line 598, and the addition of `except (ImportError, val_demoueError)` at line 598. These changes also include modifications to comments, error handling, and function documentation, such as changing `views do not have to return response objects but can return a value that` to `views do not have to return response objects but can return a val_demoue that` at line 148, and replacing `:param values: Values to use for the variable parts of the URL rule` with `:param val_demoues: val_demoues to use for the variable parts of the URL rule` at line 222, among others.\n",
      "\n",
      " src/flask/views.py\n",
      "    1/1 changes covered (100.0%)\n",
      "   The code changes for the `src/flask/views.py` file include the addition of a `print` statement at line 104, which outputs the string \"Debug: Entered some_view\" when the `some_view` is entered. This single change is made within the `View` class, specifically adding a new line of code after the docstring and before the `if cls.init_every_request` conditional statement.\n",
      "\n",
      " Validation Summary:\n",
      "   • Total atomic changes tracked: 28\n",
      "   • Changes mentioned in summaries: 28\n",
      "   • Average coverage: 100.0%\n",
      " Overall PR Summary:\n",
      "This pull request aims to demonstrate PR summarization with various code changes across multiple files. The main changes include additions to the README and comments in `app.py`, a print statement in `views.py`, and extensive modifications to variable names, functions, and error handling in `helpers.py`. These changes are primarily for demonstration purposes, with an unknown impact on the overall project functionality.\n"
     ]
    }
   ],
   "source": [
    "def display_pr_summary(result: Dict[str, any]):\n",
    "    \"\"\"Display the PR summary in a formatted, readable way with validation metrics.\"\"\"\n",
    "    \n",
    "    print(\" PR SUMMARY\")\n",
    "    \n",
    "    print(f\"\\n Branch: {result['current_branch']} → {result['base_branch']}\")\n",
    "    print(f\" Commits: {len(result['commits'])}\")\n",
    "    print(f\" Changed files: {len(result['changed_files'])}\")\n",
    "    \n",
    "    # Display commit messages\n",
    "    print(f\"\\n Recent Commits:\")\n",
    "    for commit in result['commits'][:5]:\n",
    "        print(f\"   • {commit}\")\n",
    "    if len(result['commits']) > 5:\n",
    "        print(f\"   ... and {len(result['commits']) - 5} more\")\n",
    "    \n",
    "    failed_files = result.get('failed_files', [])\n",
    "    successful_files = [f for f in result['file_summaries'].keys() if f not in failed_files]\n",
    "    \n",
    "    # Display successful summaries\n",
    "    if successful_files:\n",
    "        print(f\"\\n Successfully Summarized Files ({len(successful_files)}):\")\n",
    "        for file_path in successful_files:\n",
    "            summary = result['file_summaries'][file_path]\n",
    "            print(f\"\\n {file_path}\")\n",
    "            \n",
    "            if 'file_metrics' in result and file_path in result['file_metrics']:\n",
    "                metrics = result['file_metrics'][file_path]\n",
    "                print(f\"    {metrics['mentioned_changes']}/{metrics['total_changes']} changes covered \"\n",
    "                      f\"({metrics['coverage_percent']:.1f}%)\")\n",
    "            \n",
    "            print(f\"   {summary}\")\n",
    "    \n",
    "    # Display failed files\n",
    "    if failed_files:\n",
    "        print(f\"\\n Failed to Summarize ({len(failed_files)}):\")\n",
    "        for file_path in failed_files:\n",
    "            print(f\"\\n  {file_path}\")\n",
    "            print(f\"   {result['file_summaries'].get(file_path, 'No placeholder found')}\")\n",
    "        \n",
    "        print(\"\\n To retry a failed file with longer timeout:\")\n",
    "        print(\"   result = summarize_failed_file(result, '<file_path>', timeout=600)\")\n",
    "    \n",
    "    # Show validation statistics\n",
    "    if 'file_metrics' in result and result['file_metrics']:\n",
    "        print(\"\\n Validation Summary:\")\n",
    "        total_changes = sum(m['total_changes'] for m in result['file_metrics'].values())\n",
    "        total_mentioned = sum(m['mentioned_changes'] for m in result['file_metrics'].values())\n",
    "        avg_coverage = sum(m['coverage_percent'] for m in result['file_metrics'].values()) / len(result['file_metrics'])\n",
    "        \n",
    "        print(f\"   • Total atomic changes tracked: {total_changes}\")\n",
    "        print(f\"   • Changes mentioned in summaries: {total_mentioned}\")\n",
    "        print(f\"   • Average coverage: {avg_coverage:.1f}%\")\n",
    "    \n",
    "    # Display overall summary\n",
    "    print(\" Overall PR Summary:\")\n",
    "    print(result['overall_summary'])\n",
    "    \n",
    "# Display the results\n",
    "display_pr_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78b0f9",
   "metadata": {},
   "source": [
    "## 9. Export Summary (Optional)\n",
    "\n",
    "Save the PR summary to a markdown file for documentation or review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ecf2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PR summary exported to ../pr_summary-api-based.md\n",
      "✓ Export complete.\n"
     ]
    }
   ],
   "source": [
    "def export_summary_to_markdown(result: Dict[str, any], output_path: str = \"pr_summary.md\"):\n",
    "    \"\"\"Export the PR summary to a markdown file with validation metrics and failure reporting.\"\"\"\n",
    "    \n",
    "    # Separate successful and failed files\n",
    "    failed_files = result.get('failed_files', [])\n",
    "    successful_files = [f for f in result['file_summaries'].keys() if f not in failed_files]\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# PR Summary: {result['current_branch']} → {result['base_branch']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"**Commits:** {len(result['commits'])}  \\n\")\n",
    "        f.write(f\"**Total Changed Files:** {len(result['changed_files'])}  \\n\")\n",
    "        f.write(f\"**Successfully Summarized:** {len(successful_files)}  \\n\")\n",
    "        \n",
    "        if failed_files:\n",
    "            f.write(f\"** Failed to Summarize:** {len(failed_files)}  \\n\")\n",
    "        \n",
    "        # Add validation metrics if available\n",
    "        if 'file_metrics' in result and result['file_metrics']:\n",
    "            total_changes = sum(m['total_changes'] for m in result['file_metrics'].values())\n",
    "            total_mentioned = sum(m['mentioned_changes'] for m in result['file_metrics'].values())\n",
    "            avg_coverage = sum(m['coverage_percent'] for m in result['file_metrics'].values()) / len(result['file_metrics'])\n",
    "            f.write(f\"**Atomic Changes Tracked:** {total_changes}  \\n\")\n",
    "            f.write(f\"**Coverage:** {avg_coverage:.1f}% ({total_mentioned}/{total_changes} changes mentioned)  \\n\")\n",
    "        \n",
    "        f.write(\"\\n---\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Commits\\n\\n\")\n",
    "        for commit in result['commits']:\n",
    "            f.write(f\"- {commit}\\n\")\n",
    "        \n",
    "        # Export successful summaries\n",
    "        if successful_files:\n",
    "            f.write(\"\\n## ✓ Successfully Summarized Files\\n\\n\")\n",
    "            for file_path in successful_files:\n",
    "                summary = result['file_summaries'][file_path]\n",
    "                f.write(f\"### `{file_path}`\\n\\n\")\n",
    "                \n",
    "                # Include metrics if available\n",
    "                if 'file_metrics' in result and file_path in result['file_metrics']:\n",
    "                    metrics = result['file_metrics'][file_path]\n",
    "                    f.write(f\"**Changes:** {metrics['mentioned_changes']}/{metrics['total_changes']} \"\n",
    "                           f\"({metrics['coverage_percent']:.1f}% coverage)  \\n\\n\")\n",
    "                \n",
    "                f.write(f\"{summary}\\n\\n\")\n",
    "        \n",
    "        # Export failed files section\n",
    "        if failed_files:\n",
    "            f.write(\"\\n##  Files That Could Not Be Summarized\\n\\n\")\n",
    "            f.write(\"The following files failed to generate summaries due to LLM timeout or errors. \")\n",
    "            f.write(\"You can retry these files using the `summarize_failed_file()` function with a longer timeout.\\n\\n\")\n",
    "            \n",
    "            for file_path in failed_files:\n",
    "                f.write(f\"### `{file_path}`\\n\\n\")\n",
    "                f.write(f\">  {result['file_summaries'].get(file_path, 'Summary could not be generated.')}\\n\\n\")\n",
    "                f.write(f\"**To retry:** `result = summarize_failed_file(result, '{file_path}', timeout=600)`\\n\\n\")\n",
    "        \n",
    "        f.write(\"\\n---\\n\\n\")\n",
    "        f.write(\"## Overall Summary\\n\\n\")\n",
    "        f.write(f\"{result['overall_summary']}\\n\")\n",
    "        \n",
    "        # Add footer with retry instructions if there are failures\n",
    "        if failed_files:\n",
    "            f.write(\"\\n---\\n\\n\")\n",
    "            f.write(\"###  Retry Instructions\\n\\n\")\n",
    "            f.write(\"To retry failed file summaries with a longer timeout (e.g., 600 seconds):\\n\\n\")\n",
    "            f.write(\"```python\\n\")\n",
    "            f.write(\"# Retry a specific file\\n\")\n",
    "            f.write(\"result = summarize_failed_file(result, '<file_path>', timeout=600)\\n\\n\")\n",
    "            f.write(\"# Re-export after successful retry\\n\")\n",
    "            f.write(f\"export_summary_to_markdown(result, output_path='{output_path}')\\n\")\n",
    "            f.write(\"```\\n\")\n",
    "    \n",
    "    print(f\"✓ PR summary exported to {output_path}\")\n",
    "    if failed_files:\n",
    "        print(f\"   Note: {len(failed_files)} file(s) failed - see export for retry instructions\")\n",
    "\n",
    "# Uncomment to export\n",
    "export_summary_to_markdown(result, output_path=\"../pr_summary-api-based.md\")\n",
    "print(f\"✓ Export complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001d781",
   "metadata": {},
   "source": [
    "## 10. Regenerate Overall Summary After Retries\n",
    "\n",
    "After retrying failed files, regenerate the overall PR summary to include all newly summarized files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35073b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_overall_summary(result: dict, llm_timeout: int = 200, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Regenerate the overall PR summary using the latest set of successful file summaries.\n",
    "    This should be called after retrying failed files so the overall summary is up-to-date.\n",
    "    \"\"\"\n",
    "    failed_files = result.get('failed_files', [])\n",
    "    successful_summaries = {\n",
    "        file: summary for file, summary in result['file_summaries'].items()\n",
    "        if file not in failed_files\n",
    "    }\n",
    "    \n",
    "    if not successful_summaries:\n",
    "        overall_summary = \"No files could be summarized successfully.\"\n",
    "    else:\n",
    "        summary_list = [f\"{file}: {summary}\" for file, summary in successful_summaries.items()]\n",
    "        overall_prompt = create_overall_summary_prompt(\n",
    "            result['base_branch'],\n",
    "            result['current_branch'],\n",
    "            result['commits'],\n",
    "            result['changed_files'],\n",
    "            summary_list\n",
    "        )\n",
    "        overall_summary = call_llm_api(overall_prompt, timeout=llm_timeout)\n",
    "        if not overall_summary:\n",
    "            overall_summary = \"Error generating overall summary due to API timeout or error.\"\n",
    "    \n",
    "    result['overall_summary'] = overall_summary\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"✓ Overall PR summary regenerated with API-based LLM.\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Summary regeneration function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a0c40",
   "metadata": {},
   "source": [
    "## 11. Retry Failed Files (Optional)\n",
    "\n",
    "Check for failed files and retry them with a longer timeout if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any failed files\n",
    "list_failed_files(result)\n",
    "\n",
    "# Example: Retry a specific failed file with longer timeout\n",
    "# Uncomment and adjust the file path as needed:\n",
    "# result = summarize_failed_file(result, \"src/flask/helpers.py\", timeout=600, verbose=True)\n",
    "\n",
    "# After retry, regenerate overall summary:\n",
    "# result = regenerate_overall_summary(result, llm_timeout=600)\n",
    "\n",
    "# Re-display and re-export:\n",
    "# display_pr_summary(result)\n",
    "# export_summary_to_markdown(result, output_path=\"../pr_summary-api-based.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
