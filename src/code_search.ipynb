{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad9c566",
   "metadata": {},
   "source": [
    "# Semantic Code Search Pipeline\n",
    "This notebook implements an offline semantic code search system for the Flask repository.\n",
    "\n",
    "## Features:\n",
    "- Parse Python files using tree-sitter\n",
    "- Extract functions and classes with metadata\n",
    "- Generate embeddings using sentence-transformers (Jina v2 code embeddings)\n",
    "- Store in ChromaDB with **cosine similarity** for semantic retrieval\n",
    "- Search with natural language queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270eeed",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e88282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Import utils functions\n",
    "sys.path.append(os.getcwd())\n",
    "from utils import find_repo_root, list_python_files\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tree_sitter_languages import get_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423a900",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270c83b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev-copilot\\env\\Lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\.cache\\huggingface\\modules\\transformers_modules\\jinaai\\jina_hyphen_bert_hyphen_v2_hyphen_qk_hyphen_post_hyphen_norm\\3baf9e3ac750e76e8edd3019170176884695fb94\\configuration_bert.py:29: UserWarning: optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\n",
      "  warnings.warn(\"optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\")\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-v2-qk-post-norm:\n",
      "- modeling_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "FLASK_REPO_PATH = \"../flask\"  # Adjust to your Flask repo location\n",
    "CHROMA_DB_PATH = \"../data/chroma_db\"\n",
    "COLLECTION_NAME = \"flask_code\"\n",
    "EMBEDDING_MODEL = \"jinaai/jina-embeddings-v2-base-code\"\n",
    "\n",
    "# Initialize parser (get_parser returns a parser already configured for Python)\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL, trust_remote_code=True)\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5da476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev-copilot\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.3\n",
      "5.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)\n",
    "import transformers; print(transformers.__version__)\n",
    "import sentence_transformers; print(sentence_transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bffc890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm text1: 14.371183\n",
      "L2 norm text2: 13.882114\n",
      "Cosine similarity: 0.09143908\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding model with two very different texts\n",
    "text1 = \"This is a function to handle HTTP requests in Flask.\"\n",
    "text2 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "embeddings = embedding_model.encode([text1, text2])\n",
    "\n",
    "# print(\"Embedding for text1:\", embeddings[0])\n",
    "# print(\"Embedding for text2:\", embeddings[1])\n",
    "\n",
    "# Print summary statistics for comparison\n",
    "import numpy as np\n",
    "print(\"L2 norm text1:\", np.linalg.norm(embeddings[0]))\n",
    "print(\"L2 norm text2:\", np.linalg.norm(embeddings[1]))\n",
    "print(\"Cosine similarity:\", np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "324b47bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.25478408\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode([\"This is a test.\", \"Completely different text.\"])\n",
    "print(\"Cosine similarity:\", np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "371e2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# embedding_model = SentenceTransformer(\"Qodo/Qodo-Embed-1-1.5B\", trust_remote_code=True)\n",
    "# embeddings = embedding_model.encode([\"This is a test.\", \"Completely different text.\"])\n",
    "# import numpy as np\n",
    "# print(\"Cosine similarity:\", np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a27f4c",
   "metadata": {},
   "source": [
    "## Code Parsing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd9742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_chunks(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Extract functions and classes from a Python file using tree-sitter.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "        \n",
    "        tree = parser.parse(bytes(code, \"utf8\"))\n",
    "        root_node = tree.root_node\n",
    "        \n",
    "        def traverse(node, depth=0):\n",
    "            # Extract function definitions\n",
    "            if node.type == 'function_definition':\n",
    "                name_node = node.child_by_field_name('name')\n",
    "                if name_node:\n",
    "                    func_name = code[name_node.start_byte:name_node.end_byte]\n",
    "                    func_code = code[node.start_byte:node.end_byte]\n",
    "                    \n",
    "                    # Extract docstring if present\n",
    "                    docstring = \"\"\n",
    "                    body = node.child_by_field_name('body')\n",
    "                    if body and body.child_count > 0:\n",
    "                        first_child = body.children[0]\n",
    "                        if first_child.type == 'expression_statement':\n",
    "                            expr = first_child.children[0]\n",
    "                            if expr.type == 'string':\n",
    "                                docstring = code[expr.start_byte:expr.end_byte].strip('\"\"\"').strip(\"'''\").strip()\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        'type': 'function',\n",
    "                        'name': func_name,\n",
    "                        'code': func_code,\n",
    "                        'docstring': docstring,\n",
    "                        'file_path': file_path,\n",
    "                        'start_line': node.start_point[0] + 1,\n",
    "                        'end_line': node.end_point[0] + 1,\n",
    "                    })\n",
    "            \n",
    "            # Extract class definitions\n",
    "            elif node.type == 'class_definition':\n",
    "                name_node = node.child_by_field_name('name')\n",
    "                if name_node:\n",
    "                    class_name = code[name_node.start_byte:name_node.end_byte]\n",
    "                    class_code = code[node.start_byte:node.end_byte]\n",
    "                    \n",
    "                    # Extract class docstring\n",
    "                    docstring = \"\"\n",
    "                    body = node.child_by_field_name('body')\n",
    "                    if body and body.child_count > 0:\n",
    "                        first_child = body.children[0]\n",
    "                        if first_child.type == 'expression_statement':\n",
    "                            expr = first_child.children[0]\n",
    "                            if expr.type == 'string':\n",
    "                                docstring = code[expr.start_byte:expr.end_byte].strip('\"\"\"').strip(\"'''\").strip()\n",
    "                    \n",
    "                    # Limit class code to avoid huge chunks\n",
    "                    if len(class_code) > 2000:\n",
    "                        class_code = class_code[:2000] + \"\\n    # ... (truncated)\"\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        'type': 'class',\n",
    "                        'name': class_name,\n",
    "                        'code': class_code,\n",
    "                        'docstring': docstring,\n",
    "                        'file_path': file_path,\n",
    "                        'start_line': node.start_point[0] + 1,\n",
    "                        'end_line': node.end_point[0] + 1,\n",
    "                    })\n",
    "            \n",
    "            # Recursively traverse children\n",
    "            for child in node.children:\n",
    "                traverse(child, depth + 1)\n",
    "        \n",
    "        traverse(root_node)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b10d45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_searchable_text(chunk: Dict) -> str:\n",
    "    \"\"\"Create searchable text with prioritized metadata and limited code for better embeddings.\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # 1. Prioritize docstring (most semantic information)\n",
    "    if chunk['docstring']:\n",
    "        parts.append(f\"Documentation: {chunk['docstring']}\")\n",
    "    \n",
    "    # 2. Add type and name (critical identifiers)\n",
    "    parts.append(f\"{chunk['type']}: {chunk['name']}\")\n",
    "    \n",
    "    # 3. Add file path for context\n",
    "    file_name = chunk['file_path'].split('/')[-1] if '/' in chunk['file_path'] else chunk['file_path'].split('\\\\')[-1]\n",
    "    parts.append(f\"File: {file_name}\")\n",
    "    \n",
    "    # 4. Add limited code (first 400 chars to avoid dilution)\n",
    "    code_snippet = chunk['code'][:400]\n",
    "    parts.append(f\"Code:\\n{code_snippet}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafd58b",
   "metadata": {},
   "source": [
    "## Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ee118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_repository(repo_path: str, force_reindex: bool = False):\n",
    "    \"\"\"Index all Python files in the repository.\"\"\"\n",
    "    \n",
    "    # Initialize ChromaDB\n",
    "    os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "    \n",
    "    # Get or create collection with COSINE similarity (critical for semantic search)\n",
    "    try:\n",
    "        if force_reindex:\n",
    "            client.delete_collection(name=COLLECTION_NAME)\n",
    "            print(\"Deleted existing collection for reindexing.\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    collection = client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\n",
    "            \"description\": \"Flask repository code chunks\",\n",
    "            \"hnsw:space\": \"cosine\"  # Use cosine similarity instead of L2\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Check if already indexed\n",
    "    if collection.count() > 0 and not force_reindex:\n",
    "        print(f\"Repository already indexed with {collection.count()} chunks.\")\n",
    "        print(\"Note: If indexed before the distance metric fix, run with force_reindex=True\")\n",
    "        return collection\n",
    "    \n",
    "    # Get all Python files\n",
    "    print(f\"Finding Python files in {repo_path}...\")\n",
    "    py_files = list_python_files(repo_path)\n",
    "    print(f\"Found {len(py_files)} Python files.\")\n",
    "    \n",
    "    # Extract and index chunks\n",
    "    all_chunks = []\n",
    "    for i, file_path in enumerate(py_files):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing file {i+1}/{len(py_files)}...\")\n",
    "        \n",
    "        chunks = extract_code_chunks(file_path)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"Extracted {len(all_chunks)} code chunks.\")\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"No code chunks found!\")\n",
    "        return collection\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    print(\"Generating embeddings...\")\n",
    "    batch_size = 32\n",
    "    indexed_count = 0\n",
    "    \n",
    "    for i in range(0, len(all_chunks), batch_size):\n",
    "        batch = all_chunks[i:i+batch_size]\n",
    "        texts = [create_searchable_text(chunk) for chunk in batch]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = embedding_model.encode(texts, show_progress_bar=False)\n",
    "        \n",
    "        # Prepare unique IDs (add index to prevent collisions)\n",
    "        ids = [f\"{indexed_count + j}:{chunk['file_path']}:{chunk['name']}:{chunk['start_line']}\" \n",
    "               for j, chunk in enumerate(batch)]\n",
    "        \n",
    "        metadatas = [{\n",
    "            'type': chunk['type'],\n",
    "            'name': chunk['name'],\n",
    "            'file_path': chunk['file_path'],\n",
    "            'start_line': chunk['start_line'],\n",
    "            'end_line': chunk['end_line'],\n",
    "            'docstring': chunk['docstring'][:500] if chunk['docstring'] else \"\",\n",
    "        } for chunk in batch]\n",
    "        \n",
    "        # Store the full code (not truncated) for better context\n",
    "        documents = [chunk['code'] for chunk in batch]\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings.tolist(),\n",
    "            metadatas=metadatas,\n",
    "            documents=documents\n",
    "        )\n",
    "        \n",
    "        indexed_count += len(batch)\n",
    "        \n",
    "        if indexed_count % 100 == 0 or indexed_count == len(all_chunks):\n",
    "            print(f\"Indexed {indexed_count}/{len(all_chunks)} chunks...\")\n",
    "    \n",
    "    print(f\"✓ Indexing complete! Total chunks: {collection.count()}\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155e9a7",
   "metadata": {},
   "source": [
    "## Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e05ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_code(query: str, top_k: int = 5, apply_filter: bool = False) -> List[Dict]:\n",
    "    \"\"\"Search for code chunks matching the query with optional keyword filtering.\"\"\"\n",
    "    \n",
    "    # Connect to ChromaDB\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "    \n",
    "    try:\n",
    "        collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    except:\n",
    "        print(\"Collection not found. Please index the repository first.\")\n",
    "        return []\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Retrieve more results if filtering (to have candidates)\n",
    "    retrieve_count = top_k * 3 if apply_filter else top_k\n",
    "    \n",
    "    # Search (using cosine distance if collection was created correctly)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=retrieve_count,\n",
    "        include=['metadatas', 'documents', 'distances']\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    if results['ids'] and results['ids'][0]:\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            # For cosine distance: similarity = 1 - distance (distance is already in [0, 2])\n",
    "            # Lower distance = higher similarity\n",
    "            distance = results['distances'][0][i]\n",
    "            similarity = 1 - distance  # Cosine similarity from cosine distance\n",
    "            \n",
    "            formatted_results.append({\n",
    "                'id': results['ids'][0][i],\n",
    "                'type': results['metadatas'][0][i]['type'],\n",
    "                'name': results['metadatas'][0][i]['name'],\n",
    "                'file_path': results['metadatas'][0][i]['file_path'],\n",
    "                'start_line': results['metadatas'][0][i]['start_line'],\n",
    "                'end_line': results['metadatas'][0][i]['end_line'],\n",
    "                'docstring': results['metadatas'][0][i]['docstring'],\n",
    "                'code': results['documents'][0][i],\n",
    "                'distance': distance,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "    \n",
    "    # Apply keyword filtering if requested\n",
    "    if apply_filter and formatted_results:\n",
    "        formatted_results = filter_results_by_keywords(formatted_results, query)\n",
    "        formatted_results = formatted_results[:top_k]  # Limit to top_k after filtering\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def filter_results_by_keywords(results: List[Dict], query: str) -> List[Dict]:\n",
    "    \"\"\"Filter and re-rank results by keyword presence in name, docstring, and code.\"\"\"\n",
    "    # Extract keywords from query (simple tokenization)\n",
    "    keywords = set(query.lower().split())\n",
    "    # Remove common stop words\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'how', 'does', 'do'}\n",
    "    keywords = keywords - stop_words\n",
    "    \n",
    "    scored_results = []\n",
    "    for result in results:\n",
    "        # Create searchable text from result\n",
    "        search_text = f\"{result['name']} {result.get('docstring', '')} {result['code']}\".lower()\n",
    "        \n",
    "        # Count keyword matches\n",
    "        keyword_score = sum(1 for keyword in keywords if keyword in search_text)\n",
    "        \n",
    "        # Boost if keyword in name (very important)\n",
    "        name_score = sum(2 for keyword in keywords if keyword in result['name'].lower())\n",
    "        \n",
    "        # Boost if keyword in docstring\n",
    "        doc_score = sum(1.5 for keyword in keywords if keyword in result.get('docstring', '').lower())\n",
    "        \n",
    "        total_score = keyword_score + name_score + doc_score\n",
    "        \n",
    "        # Combine with semantic similarity (weighted)\n",
    "        combined_score = result['similarity'] * 0.6 + (total_score / max(len(keywords), 1)) * 0.4\n",
    "        \n",
    "        scored_results.append((combined_score, result))\n",
    "    \n",
    "    # Sort by combined score\n",
    "    scored_results.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return filtered results (only those with at least one keyword match)\n",
    "    filtered = [result for score, result in scored_results if score > 0]\n",
    "    \n",
    "    return filtered if filtered else [result for _, result in scored_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f594ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(results: List[Dict]):\n",
    "    \"\"\"Pretty print search results for CLI.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result['type'].upper()}: {result['name']}\")\n",
    "        print(f\"   File: {result['file_path']}:{result['start_line']}-{result['end_line']}\")\n",
    "        print(f\"   Similarity: {result['similarity']:.4f} (distance: {result['distance']:.4f})\")\n",
    "        \n",
    "        if result['docstring']:\n",
    "            # Show first 150 chars of docstring\n",
    "            doc_preview = result['docstring'][:150].replace('\\n', ' ')\n",
    "            print(f\"   Doc: {doc_preview}{'...' if len(result['docstring']) > 150 else ''}\")\n",
    "        \n",
    "        print(f\"   Code Preview:\")\n",
    "        code_lines = result['code'].split('\\n')[:8]  # Show first 8 lines\n",
    "        for line in code_lines:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                print(f\"      {line[:100]}\")  # Limit line length\n",
    "        \n",
    "        total_lines = len(result['code'].split('\\n'))\n",
    "        if total_lines > 8:\n",
    "            print(f\"      ... ({total_lines - 8} more lines)\")\n",
    "        print()  # Blank line between results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7a13f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection for reindexing.\n",
      "Finding Python files in ../flask...\n",
      "Found 34 Python files.\n",
      "Processing file 1/34...\n",
      "Processing file 11/34...\n",
      "Processing file 21/34...\n",
      "Processing file 31/34...\n",
      "Extracted 471 code chunks.\n",
      "Generating embeddings...\n",
      "Indexed 471/471 chunks...\n",
      "✓ Indexing complete! Total chunks: 471\n"
     ]
    }
   ],
   "source": [
    "# Index the repository\n",
    "# Set force_reindex=True if you indexed before the cosine similarity fix\n",
    "collection = index_repository(FLASK_REPO_PATH, force_reindex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1fe5595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: flask_code\n",
      "Total chunks: 471\n",
      "Metadata: {'hnsw:space': 'cosine', 'description': 'Flask repository code chunks'}\n",
      "Cosine similarity is ENABLED - semantic search will work correctly!\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "1. ID: 0:../flask\\examples\\celery\\src\\task_app\\tasks.py:add:8\n",
      "   Type: function, Name: add\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\tasks.py\n",
      "   Embedding dim: 768\n",
      "   Code: def add(a: int, b: int) -> int:     return a + b...\n",
      "\n",
      "2. ID: 1:../flask\\examples\\celery\\src\\task_app\\tasks.py:block:13\n",
      "   Type: function, Name: block\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\tasks.py\n",
      "   Embedding dim: 768\n",
      "   Code: def block() -> None:     time.sleep(5)...\n",
      "\n",
      "3. ID: 2:../flask\\examples\\celery\\src\\task_app\\tasks.py:process:18\n",
      "   Type: function, Name: process\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\tasks.py\n",
      "   Embedding dim: 768\n",
      "   Code: def process(self: Task, total: int) -> object:     for i in range(total):         self.update_state(...\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check collection metadata and sample entries\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "try:\n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"Collection: {collection.name}\")\n",
    "    print(f\"Total chunks: {collection.count()}\")\n",
    "    print(f\"Metadata: {collection.metadata}\")\n",
    "    \n",
    "    # Verify cosine similarity is enabled\n",
    "    if collection.metadata.get('hnsw:space') == 'cosine':\n",
    "        print(\"Cosine similarity is ENABLED - semantic search will work correctly!\")\n",
    "    else:\n",
    "        print(\"WARNING: Collection is not using cosine similarity!\")\n",
    "    \n",
    "    # Get a few samples\n",
    "    sample = collection.get(limit=3, include=['metadatas', 'documents', 'embeddings'])\n",
    "    print(f\"\\nSample entries:\")\n",
    "    for i in range(min(3, len(sample['ids']))):\n",
    "        print(f\"\\n{i+1}. ID: {sample['ids'][i]}\")\n",
    "        print(f\"   Type: {sample['metadatas'][i]['type']}, Name: {sample['metadatas'][i]['name']}\")\n",
    "        print(f\"   File: {sample['metadatas'][i]['file_path']}\")\n",
    "        \n",
    "        # Check embedding dimension\n",
    "        if sample['embeddings'] is not None and len(sample['embeddings']) > i:\n",
    "            print(f\"   Embedding dim: {len(sample['embeddings'][i])}\")\n",
    "        else:\n",
    "            print(f\"   Embedding dim: N/A\")\n",
    "        \n",
    "        # Show code snippet\n",
    "        code_preview = sample['documents'][i][:100].replace('\\n', ' ')\n",
    "        print(f\"   Code: {code_preview}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6647a1b",
   "metadata": {},
   "source": [
    "## Embedding Diagnostics\n",
    "\n",
    "Check embedding model output and verify normalization/diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5a2f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Embedding Model Output\n",
      "\n",
      "Generating embeddings for sample texts...\n",
      "\n",
      "1. Text: 'function: route decorator for Flask'\n",
      "   Embedding shape: (768,)\n",
      "   Embedding mean: 0.002063\n",
      "   Embedding std: 0.548010\n",
      "   Embedding min: -1.431866\n",
      "   Embedding max: 1.641745\n",
      "   L2 norm: 15.187018\n",
      "   Non-zero elements: 768/768\n",
      "\n",
      "2. Text: 'function: handle HTTP request'\n",
      "   Embedding shape: (768,)\n",
      "   Embedding mean: -0.001454\n",
      "   Embedding std: 0.519145\n",
      "   Embedding min: -1.617335\n",
      "   Embedding max: 1.912307\n",
      "   L2 norm: 14.387020\n",
      "   Non-zero elements: 768/768\n",
      "\n",
      "3. Text: 'class: Blueprint for Flask apps'\n",
      "   Embedding shape: (768,)\n",
      "   Embedding mean: -0.001948\n",
      "   Embedding std: 0.501446\n",
      "   Embedding min: -1.396184\n",
      "   Embedding max: 2.108231\n",
      "   L2 norm: 13.896589\n",
      "   Non-zero elements: 768/768\n",
      "\n",
      "4. Text: 'function: error handling exception'\n",
      "   Embedding shape: (768,)\n",
      "   Embedding mean: 0.001313\n",
      "   Embedding std: 0.533178\n",
      "   Embedding min: -1.307905\n",
      "   Embedding max: 1.519243\n",
      "   L2 norm: 14.775918\n",
      "   Non-zero elements: 768/768\n",
      "\n",
      "5. Text: 'simple hello world function'\n",
      "   Embedding shape: (768,)\n",
      "   Embedding mean: -0.001484\n",
      "   Embedding std: 0.515754\n",
      "   Embedding min: -1.462477\n",
      "   Embedding max: 2.271758\n",
      "   L2 norm: 14.293044\n",
      "   Non-zero elements: 768/768\n",
      "\n",
      "\n",
      "=== Pairwise Cosine Similarities ===\n",
      "\n",
      "Similarity matrix:\n",
      "  'function: route decorator for ...' <-> 'function: handle HTTP request...': 0.3202\n",
      "  'function: route decorator for ...' <-> 'class: Blueprint for Flask app...': 0.6375\n",
      "  'function: route decorator for ...' <-> 'function: error handling excep...': 0.1260\n",
      "  'function: route decorator for ...' <-> 'simple hello world function...': 0.2868\n",
      "  'function: handle HTTP request...' <-> 'class: Blueprint for Flask app...': 0.2145\n",
      "  'function: handle HTTP request...' <-> 'function: error handling excep...': 0.3530\n",
      "  'function: handle HTTP request...' <-> 'simple hello world function...': 0.2702\n",
      "  'class: Blueprint for Flask app...' <-> 'function: error handling excep...': 0.0665\n",
      "  'class: Blueprint for Flask app...' <-> 'simple hello world function...': 0.3655\n",
      "  'function: error handling excep...' <-> 'simple hello world function...': 0.1406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check Embedding Model Output\n",
    "print(\"Check Embedding Model Output\\n\")\n",
    "\n",
    "# Generate embeddings for different types of text\n",
    "sample_texts = [\n",
    "    \"function: route decorator for Flask\",\n",
    "    \"function: handle HTTP request\",\n",
    "    \"class: Blueprint for Flask apps\",\n",
    "    \"function: error handling exception\",\n",
    "    \"simple hello world function\"\n",
    "]\n",
    "\n",
    "print(\"Generating embeddings for sample texts...\")\n",
    "sample_embeddings = embedding_model.encode(sample_texts)\n",
    "\n",
    "for i, (text, emb) in enumerate(zip(sample_texts, sample_embeddings)):\n",
    "    print(f\"\\n{i+1}. Text: '{text}'\")\n",
    "    print(f\"   Embedding shape: {emb.shape}\")\n",
    "    print(f\"   Embedding mean: {np.mean(emb):.6f}\")\n",
    "    print(f\"   Embedding std: {np.std(emb):.6f}\")\n",
    "    print(f\"   Embedding min: {np.min(emb):.6f}\")\n",
    "    print(f\"   Embedding max: {np.max(emb):.6f}\")\n",
    "    print(f\"   L2 norm: {np.linalg.norm(emb):.6f}\")\n",
    "    print(f\"   Non-zero elements: {np.count_nonzero(emb)}/{len(emb)}\")\n",
    "\n",
    "# Check pairwise similarities\n",
    "print(\"\\n\\n=== Pairwise Cosine Similarities ===\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(sample_embeddings)\n",
    "print(\"\\nSimilarity matrix:\")\n",
    "for i in range(len(sample_texts)):\n",
    "    for j in range(len(sample_texts)):\n",
    "        if i < j:\n",
    "            print(f\"  '{sample_texts[i][:30]}...' <-> '{sample_texts[j][:30]}...': {similarities[i][j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acaaad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify Embedding Normalization and Diversity\n",
      "\n",
      "Analyzed 50 embeddings from the collection\n",
      "\n",
      "L2 Norms:\n",
      "  Mean: 13.255367\n",
      "  Std: 0.423480\n",
      "  Min: 12.052321\n",
      "  Max: 14.031509\n",
      "\n",
      "Pairwise Cosine Distances (sample of 20):\n",
      "  Mean: 0.566770\n",
      "  Std: 0.195600\n",
      "  Min: 0.107651\n",
      "  Max: 0.903741\n",
      "\n",
      "  Very similar pairs (distance < 0.01): 0/190\n",
      "  Embeddings show good diversity\n"
     ]
    }
   ],
   "source": [
    "# Verify Embedding Normalization and Diversity\n",
    "print(\"Verify Embedding Normalization and Diversity\\n\")\n",
    "\n",
    "# Get sample embeddings from indexed collection\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "try:\n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    sample = collection.get(limit=50, include=['embeddings', 'metadatas'])\n",
    "    \n",
    "    if sample['embeddings'] is not None and len(sample['embeddings']) > 0:\n",
    "        embeddings_array = np.array(sample['embeddings'])\n",
    "        \n",
    "        print(f\"Analyzed {len(embeddings_array)} embeddings from the collection\\n\")\n",
    "        \n",
    "        # Check norms (should be consistent for normalized embeddings)\n",
    "        norms = np.linalg.norm(embeddings_array, axis=1)\n",
    "        print(f\"L2 Norms:\")\n",
    "        print(f\"  Mean: {np.mean(norms):.6f}\")\n",
    "        print(f\"  Std: {np.std(norms):.6f}\")\n",
    "        print(f\"  Min: {np.min(norms):.6f}\")\n",
    "        print(f\"  Max: {np.max(norms):.6f}\")\n",
    "        \n",
    "        # Check diversity (pairwise distances)\n",
    "        from sklearn.metrics.pairwise import cosine_distances\n",
    "        distances = cosine_distances(embeddings_array[:20])  # Sample 20 for speed\n",
    "        \n",
    "        # Get upper triangle (unique pairs)\n",
    "        upper_triangle = distances[np.triu_indices_from(distances, k=1)]\n",
    "        \n",
    "        print(f\"\\nPairwise Cosine Distances (sample of 20):\")\n",
    "        print(f\"  Mean: {np.mean(upper_triangle):.6f}\")\n",
    "        print(f\"  Std: {np.std(upper_triangle):.6f}\")\n",
    "        print(f\"  Min: {np.min(upper_triangle):.6f}\")\n",
    "        print(f\"  Max: {np.max(upper_triangle):.6f}\")\n",
    "        \n",
    "        # Check for suspiciously similar embeddings\n",
    "        very_similar = np.sum(upper_triangle < 0.01)\n",
    "        print(f\"\\n  Very similar pairs (distance < 0.01): {very_similar}/{len(upper_triangle)}\")\n",
    "        \n",
    "        if very_similar > len(upper_triangle) * 0.1:\n",
    "            print(\"  WARNING: Many embeddings are suspiciously similar!\")\n",
    "        else:\n",
    "            print(\"  Embeddings show good diversity\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No embeddings found in collection\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f69ac1",
   "metadata": {},
   "source": [
    "## Verify Search Quality\n",
    "\n",
    "Test with a specific technical query to verify semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcb9694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: 'add route decorator'\n",
      "\n",
      "Found 3 results:\n",
      "1. FUNCTION: decorator\n",
      "   File: ../flask\\src\\flask\\sansio\\scaffold.py:360-363\n",
      "   Similarity: 0.8196 (distance: 0.1804)\n",
      "   Code Preview:\n",
      "      def decorator(f: T_route) -> T_route:\n",
      "                  endpoint = options.pop(\"endpoint\", None)\n",
      "                  self.add_url_rule(rule, endpoint, f, **options)\n",
      "                  return f\n",
      "\n",
      "2. FUNCTION: decorator\n",
      "   File: ../flask\\src\\flask\\sansio\\scaffold.py:453-455\n",
      "   Similarity: 0.6787 (distance: 0.3213)\n",
      "   Code Preview:\n",
      "      def decorator(f: F) -> F:\n",
      "                  self.view_functions[endpoint] = f\n",
      "                  return f\n",
      "\n",
      "3. FUNCTION: route\n",
      "   File: ../flask\\src\\flask\\sansio\\scaffold.py:336-365\n",
      "   Similarity: 0.6569 (distance: 0.3431)\n",
      "   Doc: Decorate a view function to register it with the given URL         rule and options. Calls :meth:`add_url_rule`, which has more         details about ...\n",
      "   Code Preview:\n",
      "      def route(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n",
      "              \"\"\"Decorate a view function to register it with the given URL\n",
      "              rule and options. Calls :meth:`add_url_rule`, which has more\n",
      "              details about the implementation.\n",
      "              .. code-block:: python\n",
      "                  @app.route(\"/\")\n",
      "      ... (22 more lines)\n",
      "\n",
      "\n",
      "Result Diversity: 2/3 unique functions/classes\n",
      "Warning: Duplicate results detected!\n"
     ]
    }
   ],
   "source": [
    "# Test query to verify semantic search is working correctly\n",
    "test_query = \"add route decorator\"\n",
    "print(f\"Test Query: '{test_query}'\\n\")\n",
    "test_results = search_code(test_query, top_k=3)\n",
    "pretty_print_results(test_results)\n",
    "\n",
    "# Check diversity - results should have different names\n",
    "if test_results:\n",
    "    names = [r['name'] for r in test_results]\n",
    "    unique_names = set(names)\n",
    "    print(f\"\\nResult Diversity: {len(unique_names)}/{len(names)} unique functions/classes\")\n",
    "    if len(unique_names) < len(names):\n",
    "        print(\"Warning: Duplicate results detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505b444",
   "metadata": {},
   "source": [
    "## Post-Filtering Comparison\n",
    "\n",
    "Compare search results with and without keyword-based post-filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb22d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Filtering Comparison\n",
      "\n",
      "Query: 'add route decorator'\n",
      "\n",
      "--- WITHOUT Post-Filtering ---\n",
      "1. FUNCTION: decorator\n",
      "   Similarity: 0.8196\n",
      "\n",
      "2. FUNCTION: decorator\n",
      "   Similarity: 0.6787\n",
      "\n",
      "3. FUNCTION: route\n",
      "   Similarity: 0.6569\n",
      "   Doc: Decorate a view function to register it with the given URL\n",
      "        rule and opti...\n",
      "\n",
      "4. FUNCTION: _method_route\n",
      "   Similarity: 0.6385\n",
      "\n",
      "5. FUNCTION: endpoint\n",
      "   Similarity: 0.6252\n",
      "   Doc: Decorate a view function to register it for the given\n",
      "        endpoint. Used if ...\n",
      "\n",
      "\n",
      "--- WITH Post-Filtering (Keyword-Based Re-ranking) ---\n",
      "1. FUNCTION: add_url_rule\n",
      "   Similarity: 0.5765\n",
      "   Doc: Register a rule for routing incoming requests and building\n",
      "        URLs. The :me...\n",
      "\n",
      "2. FUNCTION: route\n",
      "   Similarity: 0.6569\n",
      "   Doc: Decorate a view function to register it with the given URL\n",
      "        rule and opti...\n",
      "\n",
      "3. FUNCTION: decorator\n",
      "   Similarity: 0.8196\n",
      "\n",
      "4. FUNCTION: _method_route\n",
      "   Similarity: 0.6385\n",
      "\n",
      "5. FUNCTION: decorator\n",
      "   Similarity: 0.5943\n",
      "\n",
      "\n",
      "--- Comparison ---\n",
      "Unique results without filter: 4/5\n",
      "Unique results with filter: 4/5\n",
      "Results changed: 4/5 positions\n"
     ]
    }
   ],
   "source": [
    "# Post-Filtering Demonstration\n",
    "print(\"Post-Filtering Comparison\\n\")\n",
    "\n",
    "test_query = \"add route decorator\"\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "\n",
    "# Get results WITHOUT filtering\n",
    "print(\"--- WITHOUT Post-Filtering ---\")\n",
    "results_no_filter = search_code(test_query, top_k=5, apply_filter=False)\n",
    "for i, result in enumerate(results_no_filter, 1):\n",
    "    print(f\"{i}. {result['type'].upper()}: {result['name']}\")\n",
    "    print(f\"   Similarity: {result['similarity']:.4f}\")\n",
    "    if result['docstring']:\n",
    "        print(f\"   Doc: {result['docstring'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "# Get results WITH filtering\n",
    "print(\"\\n--- WITH Post-Filtering (Keyword-Based Re-ranking) ---\")\n",
    "results_with_filter = search_code(test_query, top_k=5, apply_filter=True)\n",
    "for i, result in enumerate(results_with_filter, 1):\n",
    "    print(f\"{i}. {result['type'].upper()}: {result['name']}\")\n",
    "    print(f\"   Similarity: {result['similarity']:.4f}\")\n",
    "    if result['docstring']:\n",
    "        print(f\"   Doc: {result['docstring'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "# Compare diversity\n",
    "print(\"\\n--- Comparison ---\")\n",
    "names_no_filter = [r['name'] for r in results_no_filter]\n",
    "names_with_filter = [r['name'] for r in results_with_filter]\n",
    "\n",
    "print(f\"Unique results without filter: {len(set(names_no_filter))}/{len(names_no_filter)}\")\n",
    "print(f\"Unique results with filter: {len(set(names_with_filter))}/{len(names_with_filter)}\")\n",
    "\n",
    "# Check if results changed\n",
    "changed = sum(1 for i in range(min(len(names_no_filter), len(names_with_filter))) \n",
    "              if names_no_filter[i] != names_with_filter[i])\n",
    "print(f\"Results changed: {changed}/{min(len(names_no_filter), len(names_with_filter))} positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a094a",
   "metadata": {},
   "source": [
    "## Similarity Score Distribution Analysis\n",
    "\n",
    "Visualize and analyze the distribution of similarity scores for sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Similarity Score Distribution Analysis ===\n",
      "\n",
      "Query: 'add route decorator'\n",
      "  Results: 20\n",
      "  Similarity range: [0.5239, 0.8196]\n",
      "  Mean: 0.5992\n",
      "  Std: 0.0630\n",
      "  Top 3: 0.8196, 0.6787, 0.6569\n",
      "  Bottom 3: 0.5487, 0.5348, 0.5239\n",
      "\n",
      "Query: 'handle HTTP request'\n",
      "  Results: 20\n",
      "  Similarity range: [0.2643, 0.5321]\n",
      "  Mean: 0.3269\n",
      "  Std: 0.0695\n",
      "  Top 3: 0.5321, 0.4264, 0.4036\n",
      "  Bottom 3: 0.2683, 0.2675, 0.2643\n",
      "\n",
      "Query: 'template rendering'\n",
      "  Results: 20\n",
      "  Similarity range: [0.3883, 0.6051]\n",
      "  Mean: 0.4327\n",
      "  Std: 0.0542\n",
      "  Top 3: 0.6051, 0.5135, 0.5109\n",
      "  Bottom 3: 0.3936, 0.3908, 0.3883\n",
      "\n",
      "Query: 'error handling'\n",
      "  Results: 20\n",
      "  Similarity range: [0.2632, 0.4777]\n",
      "  Mean: 0.3256\n",
      "  Std: 0.0512\n",
      "  Top 3: 0.4777, 0.4057, 0.3898\n",
      "  Bottom 3: 0.2740, 0.2725, 0.2632\n",
      "\n",
      "Query: 'database connection'\n",
      "  Results: 20\n",
      "  Similarity range: [0.1732, 0.5466]\n",
      "  Mean: 0.2703\n",
      "  Std: 0.0990\n",
      "  Top 3: 0.5466, 0.4227, 0.3944\n",
      "  Bottom 3: 0.1740, 0.1739, 0.1732\n",
      "\n",
      "\n",
      "=== Overall Statistics (all 100 results) ===\n",
      "Mean similarity: 0.3909\n",
      "Std similarity: 0.1358\n",
      "Min similarity: 0.1732\n",
      "Max similarity: 0.8196\n",
      "\n",
      "Distribution:\n",
      "  [0.0-0.2]: 5 results (5.0%)\n",
      "  [0.2-0.4]: 56 results (56.0%)\n",
      "  [0.4-0.6]: 32 results (32.0%)\n",
      "  [0.6-0.8]: 6 results (6.0%)\n",
      "  [0.8-1.0]: 1 results (1.0%)\n",
      "\n",
      "✓ Similarity distribution looks reasonable\n"
     ]
    }
   ],
   "source": [
    "# Analyze Similarity Score Distribution\n",
    "print(\"=== Similarity Score Distribution Analysis ===\\n\")\n",
    "\n",
    "# Test with multiple queries\n",
    "test_queries = [\n",
    "    \"add route decorator\",\n",
    "    \"handle HTTP request\",\n",
    "    \"template rendering\",\n",
    "    \"error handling\",\n",
    "    \"database connection\"\n",
    "]\n",
    "\n",
    "all_similarities = []\n",
    "query_stats = []\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search_code(query, top_k=20)\n",
    "    if results:\n",
    "        similarities = [r['similarity'] for r in results]\n",
    "        all_similarities.extend(similarities)\n",
    "        \n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(f\"  Results: {len(results)}\")\n",
    "        print(f\"  Similarity range: [{min(similarities):.4f}, {max(similarities):.4f}]\")\n",
    "        print(f\"  Mean: {np.mean(similarities):.4f}\")\n",
    "        print(f\"  Std: {np.std(similarities):.4f}\")\n",
    "        \n",
    "        # Show top 3 and bottom 3\n",
    "        print(f\"  Top 3: {', '.join([f'{s:.4f}' for s in similarities[:3]])}\")\n",
    "        print(f\"  Bottom 3: {', '.join([f'{s:.4f}' for s in similarities[-3:]])}\")\n",
    "        print()\n",
    "        \n",
    "        query_stats.append({\n",
    "            'query': query,\n",
    "            'mean': np.mean(similarities),\n",
    "            'std': np.std(similarities),\n",
    "            'range': max(similarities) - min(similarities)\n",
    "        })\n",
    "\n",
    "# Overall statistics\n",
    "if all_similarities:\n",
    "    print(f\"\\n=== Overall Statistics (all {len(all_similarities)} results) ===\")\n",
    "    print(f\"Mean similarity: {np.mean(all_similarities):.4f}\")\n",
    "    print(f\"Std similarity: {np.std(all_similarities):.4f}\")\n",
    "    print(f\"Min similarity: {np.min(all_similarities):.4f}\")\n",
    "    print(f\"Max similarity: {np.max(all_similarities):.4f}\")\n",
    "    \n",
    "    # Distribution bins\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    hist, _ = np.histogram(all_similarities, bins=bins)\n",
    "    print(f\"\\nDistribution:\")\n",
    "    for i in range(len(bins)-1):\n",
    "        print(f\"  [{bins[i]:.1f}-{bins[i+1]:.1f}]: {hist[i]} results ({hist[i]/len(all_similarities)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for saturation\n",
    "    if np.mean(all_similarities) > 0.95:\n",
    "        print(\"\\nWARNING: Similarities are saturated near 1.0!\")\n",
    "        print(\"   This suggests the model is not differentiating well between chunks.\")\n",
    "    elif np.std(all_similarities) < 0.05:\n",
    "        print(\"\\nWARNING: Very low variance in similarities!\")\n",
    "        print(\"   Results may not be well-ranked.\")\n",
    "    else:\n",
    "        print(\"\\n✓ Similarity distribution looks reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4185ea1",
   "metadata": {},
   "source": [
    "## Example Searches\n",
    "Try different queries to test the semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5274169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how does Flask handle routing\n",
      "Found 5 results:\n",
      "1. FUNCTION: routes_command\n",
      "   File: ../flask\\src\\flask\\cli.py:1061-1107\n",
      "   Similarity: 0.6058 (distance: 0.3942)\n",
      "   Doc: Show all registered routes with endpoints and methods.\n",
      "   Code Preview:\n",
      "      def routes_command(sort: str, all_methods: bool) -> None:\n",
      "          \"\"\"Show all registered routes with endpoints and methods.\"\"\"\n",
      "          rules = list(current_app.url_map.iter_rules())\n",
      "          if not rules:\n",
      "              click.echo(\"No routes were registered.\")\n",
      "              return\n",
      "      ... (39 more lines)\n",
      "\n",
      "2. FUNCTION: index\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\__init__.py:20-21\n",
      "   Similarity: 0.5737 (distance: 0.4263)\n",
      "   Code Preview:\n",
      "      def index() -> str:\n",
      "              return render_template(\"index.html\")\n",
      "\n",
      "3. FUNCTION: match_request\n",
      "   File: ../flask\\src\\flask\\ctx.py:398-407\n",
      "   Similarity: 0.5336 (distance: 0.4664)\n",
      "   Doc: Apply routing to the current request, storing either the matched         endpoint and args, or a routing exception.\n",
      "   Code Preview:\n",
      "      def match_request(self) -> None:\n",
      "              \"\"\"Apply routing to the current request, storing either the matched\n",
      "              endpoint and args, or a routing exception.\n",
      "              \"\"\"\n",
      "              try:\n",
      "                  result = self.url_adapter.match(return_rule=True)  # type: ignore[union-attr]\n",
      "              except HTTPException as e:\n",
      "                  self._request.routing_exception = e  # type: ignore[union-attr]\n",
      "      ... (2 more lines)\n",
      "\n",
      "4. CLASS: MethodView\n",
      "   File: ../flask\\src\\flask\\views.py:138-191\n",
      "   Similarity: 0.5323 (distance: 0.4677)\n",
      "   Doc: Dispatches request methods to the corresponding instance methods.     For example, if you implement a ``get`` method, it will be used to     handle ``...\n",
      "   Code Preview:\n",
      "      class MethodView(View):\n",
      "          \"\"\"Dispatches request methods to the corresponding instance methods.\n",
      "          For example, if you implement a ``get`` method, it will be used to\n",
      "          handle ``GET`` requests.\n",
      "          This can be useful for defining a REST API.\n",
      "          :attr:`methods` is automatically set based on the methods defined on\n",
      "      ... (46 more lines)\n",
      "\n",
      "5. FUNCTION: app\n",
      "   File: ../flask\\src\\flask\\cli.py:963-966\n",
      "   Similarity: 0.5301 (distance: 0.4699)\n",
      "   Code Preview:\n",
      "      def app(\n",
      "                      environ: WSGIEnvironment, start_response: StartResponse\n",
      "                  ) -> cabc.Iterable[bytes]:\n",
      "                      raise err from None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Routing\n",
    "query = \"how does Flask handle routing\"\n",
    "print(f\"Query: {query}\")\n",
    "results = search_code(query, top_k=5)\n",
    "pretty_print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc5c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: request context and session management\n",
      "Found 5 results:\n",
      "1. FUNCTION: session\n",
      "   File: ../flask\\src\\flask\\ctx.py:381-396\n",
      "   Similarity: 0.5481 (distance: 0.4519)\n",
      "   Doc: The session object associated with this context. Accessed through         :data:`.session`. Only available in request contexts, otherwise raises      ...\n",
      "   Code Preview:\n",
      "      def session(self) -> SessionMixin:\n",
      "              \"\"\"The session object associated with this context. Accessed through\n",
      "              :data:`.session`. Only available in request contexts, otherwise raises\n",
      "              :exc:`RuntimeError`.\n",
      "              \"\"\"\n",
      "              if self._request is None:\n",
      "                  raise RuntimeError(\"There is no request in this context.\")\n",
      "      ... (8 more lines)\n",
      "\n",
      "2. FUNCTION: open_session\n",
      "   File: ../flask\\src\\flask\\sessions.py:337-349\n",
      "   Similarity: 0.5346 (distance: 0.4654)\n",
      "   Code Preview:\n",
      "      def open_session(self, app: Flask, request: Request) -> SecureCookieSession | None:\n",
      "              s = self.get_signing_serializer(app)\n",
      "              if s is None:\n",
      "                  return None\n",
      "              val = request.cookies.get(self.get_cookie_name(app))\n",
      "              if not val:\n",
      "                  return self.session_class()\n",
      "              max_age = int(app.permanent_session_lifetime.total_seconds())\n",
      "      ... (5 more lines)\n",
      "\n",
      "3. FUNCTION: _default_template_ctx_processor\n",
      "   File: ../flask\\src\\flask\\templating.py:21-32\n",
      "   Similarity: 0.5287 (distance: 0.4713)\n",
      "   Doc: Default template context processor.  Injects `request`,     `session` and `g`.\n",
      "   Code Preview:\n",
      "      def _default_template_ctx_processor() -> dict[str, t.Any]:\n",
      "          \"\"\"Default template context processor.  Injects `request`,\n",
      "          `session` and `g`.\n",
      "          \"\"\"\n",
      "          ctx = app_ctx._get_current_object()\n",
      "          rv: dict[str, t.Any] = {\"g\": ctx.g}\n",
      "          if ctx.has_request:\n",
      "      ... (4 more lines)\n",
      "\n",
      "4. CLASS: _AppCtxGlobalsProxy\n",
      "   File: ../flask\\src\\flask\\globals.py:26-26\n",
      "   Similarity: 0.5250 (distance: 0.4750)\n",
      "   Code Preview:\n",
      "      class _AppCtxGlobalsProxy(ProxyMixin[_AppCtxGlobals], _AppCtxGlobals): ...\n",
      "\n",
      "5. FUNCTION: save_session\n",
      "   File: ../flask\\src\\flask\\sessions.py:351-399\n",
      "   Similarity: 0.5101 (distance: 0.4899)\n",
      "   Code Preview:\n",
      "      def save_session(\n",
      "              self, app: Flask, session: SessionMixin, response: Response\n",
      "          ) -> None:\n",
      "              name = self.get_cookie_name(app)\n",
      "              domain = self.get_cookie_domain(app)\n",
      "              path = self.get_cookie_path(app)\n",
      "              secure = self.get_cookie_secure(app)\n",
      "              partitioned = self.get_cookie_partitioned(app)\n",
      "      ... (41 more lines)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Request handling\n",
    "query = \"request context and session management\"\n",
    "print(f\"Query: {query}\")\n",
    "results = search_code(query, top_k=5)\n",
    "pretty_print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9300728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: template rendering and Jinja integration\n",
      "Found 5 results:\n",
      "1. FUNCTION: _render\n",
      "   File: ../flask\\src\\flask\\templating.py:122-132\n",
      "   Similarity: 0.6581 (distance: 0.3419)\n",
      "   Code Preview:\n",
      "      def _render(ctx: AppContext, template: Template, context: dict[str, t.Any]) -> str:\n",
      "          app = ctx.app\n",
      "          app.update_template_context(ctx, context)\n",
      "          before_render_template.send(\n",
      "              app, _async_wrapper=app.ensure_sync, template=template, context=context\n",
      "          )\n",
      "          rv = template.render(context)\n",
      "          template_rendered.send(\n",
      "      ... (3 more lines)\n",
      "\n",
      "2. CLASS: DispatchingJinjaLoader\n",
      "   File: ../flask\\src\\flask\\templating.py:48-119\n",
      "   Similarity: 0.6288 (distance: 0.3712)\n",
      "   Doc: A loader that looks for templates in the application and all     the blueprint folders.\n",
      "   Code Preview:\n",
      "      class DispatchingJinjaLoader(BaseLoader):\n",
      "          \"\"\"A loader that looks for templates in the application and all\n",
      "          the blueprint folders.\n",
      "          \"\"\"\n",
      "          def __init__(self, app: App) -> None:\n",
      "              self.app = app\n",
      "      ... (48 more lines)\n",
      "\n",
      "3. CLASS: Environment\n",
      "   File: ../flask\\src\\flask\\templating.py:35-45\n",
      "   Similarity: 0.6186 (distance: 0.3814)\n",
      "   Doc: Works like a regular Jinja environment but has some additional     knowledge of how Flask's blueprint works so that it can prepend the     name of the...\n",
      "   Code Preview:\n",
      "      class Environment(BaseEnvironment):\n",
      "          \"\"\"Works like a regular Jinja environment but has some additional\n",
      "          knowledge of how Flask's blueprint works so that it can prepend the\n",
      "          name of the blueprint to referenced templates if necessary.\n",
      "          \"\"\"\n",
      "          def __init__(self, app: App, **options: t.Any) -> None:\n",
      "              if \"loader\" not in options:\n",
      "      ... (3 more lines)\n",
      "\n",
      "4. FUNCTION: generate\n",
      "   File: ../flask\\src\\flask\\templating.py:171-175\n",
      "   Similarity: 0.5861 (distance: 0.4139)\n",
      "   Code Preview:\n",
      "      def generate() -> t.Iterator[str]:\n",
      "              yield from template.generate(context)\n",
      "              template_rendered.send(\n",
      "                  app, _async_wrapper=app.ensure_sync, template=template, context=context\n",
      "              )\n",
      "\n",
      "5. FUNCTION: create_jinja_environment\n",
      "   File: ../flask\\src\\flask\\sansio\\app.py:476-477\n",
      "   Similarity: 0.5780 (distance: 0.4220)\n",
      "   Code Preview:\n",
      "      def create_jinja_environment(self) -> Environment:\n",
      "              raise NotImplementedError()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Template rendering\n",
    "query = \"template rendering and Jinja integration\"\n",
    "print(f\"Query: {query}\")\n",
    "results = search_code(query, top_k=5)\n",
    "pretty_print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "933a9ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: error handling and exception management\n",
      "Found 5 results:\n",
      "1. FUNCTION: on_json_loading_failed\n",
      "   File: ../flask\\src\\flask\\wrappers.py:212-219\n",
      "   Similarity: 0.4673 (distance: 0.5327)\n",
      "   Code Preview:\n",
      "      def on_json_loading_failed(self, e: ValueError | None) -> t.Any:\n",
      "              try:\n",
      "                  return super().on_json_loading_failed(e)\n",
      "              except BadRequest as ebr:\n",
      "                  if current_app and current_app.debug:\n",
      "                      raise\n",
      "                  raise BadRequest() from ebr\n",
      "\n",
      "2. FUNCTION: __exit__\n",
      "   File: ../flask\\src\\flask\\ctx.py:486-492\n",
      "   Similarity: 0.3990 (distance: 0.6010)\n",
      "   Code Preview:\n",
      "      def __exit__(\n",
      "              self,\n",
      "              exc_type: type[BaseException] | None,\n",
      "              exc_value: BaseException | None,\n",
      "              tb: TracebackType | None,\n",
      "          ) -> None:\n",
      "              self.pop(exc_value)\n",
      "\n",
      "3. CLASS: UnexpectedUnicodeError\n",
      "   File: ../flask\\src\\flask\\debughelpers.py:17-20\n",
      "   Similarity: 0.3956 (distance: 0.6044)\n",
      "   Doc: Raised in places where we want some better error reporting for     unexpected unicode or binary data.\n",
      "   Code Preview:\n",
      "      class UnexpectedUnicodeError(AssertionError, UnicodeError):\n",
      "          \"\"\"Raised in places where we want some better error reporting for\n",
      "          unexpected unicode or binary data.\n",
      "          \"\"\"\n",
      "\n",
      "4. FUNCTION: handle_user_exception\n",
      "   File: ../flask\\src\\flask\\app.py:864-894\n",
      "   Similarity: 0.3868 (distance: 0.6132)\n",
      "   Doc: This method is called whenever an exception occurs that         should be handled. A special case is :class:`~werkzeug         .exceptions.HTTPExcepti...\n",
      "   Code Preview:\n",
      "      def handle_user_exception(\n",
      "              self, ctx: AppContext, e: Exception\n",
      "          ) -> HTTPException | ft.ResponseReturnValue:\n",
      "              \"\"\"This method is called whenever an exception occurs that\n",
      "              should be handled. A special case is :class:`~werkzeug\n",
      "              .exceptions.HTTPException` which is forwarded to the\n",
      "              :meth:`handle_http_exception` method. This function will either\n",
      "              return a response value or reraise the exception with the same\n",
      "      ... (23 more lines)\n",
      "\n",
      "5. FUNCTION: _fail\n",
      "   File: ../flask\\src\\flask\\sessions.py:103-108\n",
      "   Similarity: 0.3691 (distance: 0.6309)\n",
      "   Code Preview:\n",
      "      def _fail(self, *args: t.Any, **kwargs: t.Any) -> t.NoReturn:\n",
      "              raise RuntimeError(\n",
      "                  \"The session is unavailable because no secret \"\n",
      "                  \"key was set.  Set the secret_key on the \"\n",
      "                  \"application to something unique and secret.\"\n",
      "              )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Error handling\n",
    "query = \"error handling and exception management\"\n",
    "print(f\"Query: {query}\")\n",
    "results = search_code(query, top_k=5)\n",
    "pretty_print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90144127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: error handling and exception management\n",
      "Found 5 results:\n",
      "1. FUNCTION: handle_user_exception\n",
      "   File: ../flask\\src\\flask\\app.py:864-894\n",
      "   Similarity: 0.3868 (distance: 0.6132)\n",
      "   Doc: This method is called whenever an exception occurs that         should be handled. A special case is :class:`~werkzeug         .exceptions.HTTPExcepti...\n",
      "   Code Preview:\n",
      "      def handle_user_exception(\n",
      "              self, ctx: AppContext, e: Exception\n",
      "          ) -> HTTPException | ft.ResponseReturnValue:\n",
      "              \"\"\"This method is called whenever an exception occurs that\n",
      "              should be handled. A special case is :class:`~werkzeug\n",
      "              .exceptions.HTTPException` which is forwarded to the\n",
      "              :meth:`handle_http_exception` method. This function will either\n",
      "              return a response value or reraise the exception with the same\n",
      "      ... (23 more lines)\n",
      "\n",
      "2. FUNCTION: handle_http_exception\n",
      "   File: ../flask\\src\\flask\\app.py:829-862\n",
      "   Similarity: 0.3406 (distance: 0.6594)\n",
      "   Doc: Handles an HTTP exception.  By default this will invoke the         registered error handlers and fall back to returning the         exception as resp...\n",
      "   Code Preview:\n",
      "      def handle_http_exception(\n",
      "              self, ctx: AppContext, e: HTTPException\n",
      "          ) -> HTTPException | ft.ResponseReturnValue:\n",
      "              \"\"\"Handles an HTTP exception.  By default this will invoke the\n",
      "              registered error handlers and fall back to returning the\n",
      "              exception as response.\n",
      "              .. versionchanged:: 1.0.3\n",
      "      ... (26 more lines)\n",
      "\n",
      "3. FUNCTION: register_error_handler\n",
      "   File: ../flask\\src\\flask\\sansio\\scaffold.py:642-654\n",
      "   Similarity: 0.3205 (distance: 0.6795)\n",
      "   Doc: Alternative error attach function to the :meth:`errorhandler`         decorator that is more straightforward to use for non decorator         usage.  ...\n",
      "   Code Preview:\n",
      "      def register_error_handler(\n",
      "              self,\n",
      "              code_or_exception: type[Exception] | int,\n",
      "              f: ft.ErrorHandlerCallable,\n",
      "          ) -> None:\n",
      "              \"\"\"Alternative error attach function to the :meth:`errorhandler`\n",
      "              decorator that is more straightforward to use for non decorator\n",
      "              usage.\n",
      "      ... (5 more lines)\n",
      "\n",
      "4. CLASS: UnexpectedUnicodeError\n",
      "   File: ../flask\\src\\flask\\debughelpers.py:17-20\n",
      "   Similarity: 0.3956 (distance: 0.6044)\n",
      "   Doc: Raised in places where we want some better error reporting for     unexpected unicode or binary data.\n",
      "   Code Preview:\n",
      "      class UnexpectedUnicodeError(AssertionError, UnicodeError):\n",
      "          \"\"\"Raised in places where we want some better error reporting for\n",
      "          unexpected unicode or binary data.\n",
      "          \"\"\"\n",
      "\n",
      "5. CLASS: DebugFilesKeyError\n",
      "   File: ../flask\\src\\flask\\debughelpers.py:23-47\n",
      "   Similarity: 0.3495 (distance: 0.6505)\n",
      "   Doc: Raised from request.files during debugging.  The idea is that it can     provide a better error message than just a generic KeyError/BadRequest.\n",
      "   Code Preview:\n",
      "      class DebugFilesKeyError(KeyError, AssertionError):\n",
      "          \"\"\"Raised from request.files during debugging.  The idea is that it can\n",
      "          provide a better error message than just a generic KeyError/BadRequest.\n",
      "          \"\"\"\n",
      "          def __init__(self, request: Request, key: str) -> None:\n",
      "              form_matches = request.form.getlist(key)\n",
      "              buf = [\n",
      "      ... (17 more lines)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Error handling\n",
    "query = \"error handling and exception management\"\n",
    "print(f\"Query: {query}\")\n",
    "results = search_code(query, top_k=5, apply_filter=True)\n",
    "pretty_print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85295c96",
   "metadata": {},
   "source": [
    "## Custom Search\n",
    "Run your own queries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97e48471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 results:\n",
      "1. FUNCTION: get_db\n",
      "   File: ../flask\\examples\\tutorial\\flaskr\\db.py:9-20\n",
      "   Similarity: 0.6040 (distance: 0.3960)\n",
      "   Doc: Connect to the application's configured database. The connection     is unique for each request and will be reused if this is called     again.\n",
      "   Code Preview:\n",
      "      def get_db():\n",
      "          \"\"\"Connect to the application's configured database. The connection\n",
      "          is unique for each request and will be reused if this is called\n",
      "          again.\n",
      "          \"\"\"\n",
      "          if \"db\" not in g:\n",
      "              g.db = sqlite3.connect(\n",
      "                  current_app.config[\"DATABASE\"], detect_types=sqlite3.PARSE_DECLTYPES\n",
      "      ... (4 more lines)\n",
      "\n",
      "2. FUNCTION: close_db\n",
      "   File: ../flask\\examples\\tutorial\\flaskr\\db.py:23-30\n",
      "   Similarity: 0.5232 (distance: 0.4768)\n",
      "   Doc: If this request connected to the database, close the     connection.\n",
      "   Code Preview:\n",
      "      def close_db(e=None):\n",
      "          \"\"\"If this request connected to the database, close the\n",
      "          connection.\n",
      "          \"\"\"\n",
      "          db = g.pop(\"db\", None)\n",
      "          if db is not None:\n",
      "              db.close()\n",
      "\n",
      "3. FUNCTION: init_app\n",
      "   File: ../flask\\examples\\tutorial\\flaskr\\db.py:51-56\n",
      "   Similarity: 0.4266 (distance: 0.5734)\n",
      "   Doc: Register database functions with the Flask app. This is called by     the application factory.\n",
      "   Code Preview:\n",
      "      def init_app(app):\n",
      "          \"\"\"Register database functions with the Flask app. This is called by\n",
      "          the application factory.\n",
      "          \"\"\"\n",
      "          app.teardown_appcontext(close_db)\n",
      "          app.cli.add_command(init_db_command)\n",
      "\n",
      "4. FUNCTION: create_app\n",
      "   File: ../flask\\examples\\tutorial\\flaskr\\__init__.py:6-51\n",
      "   Similarity: 0.3768 (distance: 0.6232)\n",
      "   Doc: Create and configure an instance of the Flask application.\n",
      "   Code Preview:\n",
      "      def create_app(test_config=None):\n",
      "          \"\"\"Create and configure an instance of the Flask application.\"\"\"\n",
      "          app = Flask(__name__, instance_relative_config=True)\n",
      "          app.config.from_mapping(\n",
      "              # a default secret that should be overridden by instance config\n",
      "              SECRET_KEY=\"dev\",\n",
      "              # store the database in the instance folder\n",
      "              DATABASE=os.path.join(app.instance_path, \"flaskr.sqlite\"),\n",
      "      ... (38 more lines)\n",
      "\n",
      "5. CLASS: FlaskTask\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\__init__.py:30-33\n",
      "   Similarity: 0.3650 (distance: 0.6350)\n",
      "   Code Preview:\n",
      "      class FlaskTask(Task):\n",
      "              def __call__(self, *args: object, **kwargs: object) -> object:\n",
      "                  with app.app_context():\n",
      "                      return self.run(*args, **kwargs)\n",
      "\n",
      "6. CLASS: FlaskProxy\n",
      "   File: ../flask\\src\\flask\\globals.py:22-22\n",
      "   Similarity: 0.3384 (distance: 0.6616)\n",
      "   Code Preview:\n",
      "      class FlaskProxy(ProxyMixin[Flask], Flask): ...\n",
      "\n",
      "7. FUNCTION: create_app\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\__init__.py:7-26\n",
      "   Similarity: 0.3936 (distance: 0.6064)\n",
      "   Code Preview:\n",
      "      def create_app() -> Flask:\n",
      "          app = Flask(__name__)\n",
      "          app.config.from_mapping(\n",
      "              CELERY=dict(\n",
      "                  broker_url=\"redis://localhost\",\n",
      "                  result_backend=\"redis://localhost\",\n",
      "                  task_ignore_result=True,\n",
      "              ),\n",
      "      ... (12 more lines)\n",
      "\n",
      "8. FUNCTION: __enter__\n",
      "   File: ../flask\\src\\flask\\testing.py:249-253\n",
      "   Similarity: 0.3922 (distance: 0.6078)\n",
      "   Code Preview:\n",
      "      def __enter__(self) -> FlaskClient:\n",
      "              if self.preserve_context:\n",
      "                  raise RuntimeError(\"Cannot nest client invocations\")\n",
      "              self.preserve_context = True\n",
      "              return self\n",
      "\n",
      "9. FUNCTION: celery_init_app\n",
      "   File: ../flask\\examples\\celery\\src\\task_app\\__init__.py:29-39\n",
      "   Similarity: 0.3456 (distance: 0.6544)\n",
      "   Code Preview:\n",
      "      def celery_init_app(app: Flask) -> Celery:\n",
      "          class FlaskTask(Task):\n",
      "              def __call__(self, *args: object, **kwargs: object) -> object:\n",
      "                  with app.app_context():\n",
      "                      return self.run(*args, **kwargs)\n",
      "          celery_app = Celery(app.name, task_cls=FlaskTask)\n",
      "          celery_app.config_from_object(app.config[\"CELERY\"])\n",
      "      ... (3 more lines)\n",
      "\n",
      "10. FUNCTION: __init__\n",
      "   File: ../flask\\src\\flask\\ctx.py:299-336\n",
      "   Similarity: 0.3390 (distance: 0.6610)\n",
      "   Code Preview:\n",
      "      def __init__(\n",
      "              self,\n",
      "              app: Flask,\n",
      "              *,\n",
      "              request: Request | None = None,\n",
      "              session: SessionMixin | None = None,\n",
      "          ) -> None:\n",
      "              self.app = app\n",
      "      ... (30 more lines)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom search - modify the query below\n",
    "custom_query = \"database connection pooling in Flask\"\n",
    "results = search_code(custom_query, top_k=10, apply_filter=True)\n",
    "pretty_print_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
